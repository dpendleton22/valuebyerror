{
  
    
        "post0": {
            "title": "Title",
            "content": "Data Science Interviews . &quot;I&#39;ve created, facilated and taken data science interviews...I just have a few cents on the process&quot; . toc:true- branch: master | badges: true | comments: true | categories: [data science interviews, interviewing] | image: https://media.giphy.com/media/GIQkxve3gvNHG/giphy.gif | . Candidates must be compared but I have some thoughts on the current methods I&#39;ve seen in the field: . Interviews should match the job day to day | Why are interviews computer science algorithm courses? (unless using BFS at work it shouldn&#39;t be tested) | Do we really need to code under a clock? I&#39;ve never missed a project due to needing an extra 30 min. Oh yeah, and EVERYONE uses stackoverflow and google in some capacity | . My favorite interviews: . Open ended problem solving projects. It tests my ability to meet a deadline, use my imagination and explain my process but these are also some of the most difficult interviews to create....I&#39;ve attempted it before | . My Dumpster Fire: Creating an interview . My team let me lead an interview process for a data science researcher. I made the job description, set the qualifications and created the code interview. This is how it went and what I learned . The Job . Research data scientist | . What we were looking for in a candidate . Self starter and highly self motivated developer | Knows enough to be dangerous but also shows the ability to learn | Experienced in python or another programming language | Has some research experience | . What I was trying to do: . Create an interview process to determine the right candidates for our research group. | Type of interview: Project (1 week) | Data: 3 Open sourced sets from Kaggle (candidate selected one) | Task: Provide analysis of data and how a model would be used to solve a potential problem in that industry | . What actually happened: . The removal of creating and implementing a model set the bar too low. By the fourth candidate, we saw very similar analysis presentations | Finding an open sourced dataset was harder than anticpated | Some candidates were confused by our laxed instructions. Saying &quot;Here&#39;s some data, explore it, and tell us what you&#39;d do to it&quot; left some candidated asking for direction | Only a handful of candidates actually took advantage of pulling in more open sourced data | . When Interviewing . Don&#39;t take it personal. You may think you have work to do but data science interviews also need some fixing | Show some initiative! Start a self-project, you&#39;ll be able to show it in every interview | If you have an algorithm on your resume, you should be able to speak to it | .",
            "url": "https://dpendleton22.github.io/valuebyerror/2021/06/01/_WaxOnWaxOff.html",
            "relUrl": "/2021/06/01/_WaxOnWaxOff.html",
            "date": " • Jun 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Data Science Interviews",
            "content": "&quot;I&#39;ve created, facilated and taken data science interviews...I just have a few cents on the process&quot; . What This Post is About: . Data science interviews and their process....that&#39;s it (This isn&#39;t about computer science based interviews) . Interviews Should . Match the job day to day | Be project based over a few days | Allow the interviewer to show their imagination (a number of data science solutions and break throughs are not complex but clever) | . . Interviews Should Not: . Be computer science algorithm based? (unless using BFS at work it shouldn&#39;t be tested) | Involve coding under a clock? I&#39;ve never missed a project due to needing an extra 30 min. Oh yeah, and EVERYONE uses stackoverflow and google in some capacity | . My favorite interviews: . Open ended problem solving projects. It tests my ability to meet a deadline, use my imagination and explain my process but these are also some of the most difficult interviews to create....I&#39;ve attempted it before | . My Dumpster Fire: Creating an interview . My team let me lead an interview process for a data science researcher. I made the job description, set the qualifications and created the code interview. This is how it went and what I learned . The Job . Research data scientist | . What we were looking for in a candidate . Self starter and highly self motivated developer | Knows enough to be dangerous but also shows the ability to learn | Experienced in python or another programming language | Has some research experience | . What I was trying to do: . Create an interview process to determine the right candidates for our research group | Type of interview: Project (1 week) | Data: 3 Open sourced sets from Kaggle (candidate selected one) | Task: Provide analysis of data and how a model would be used to solve a potential problem in that industry | . What actually happened: . The removal of creating and implementing a model set the bar too low. By the fourth candidate, we saw very similar analysis presentations | Finding an open sourced dataset was harder than anticpated | Some candidates were confused by our laxed instructions. Saying &quot;Here&#39;s some data, explore it, and tell us what you&#39;d do to it&quot; left some candidated asking for direction | Only a handful of candidates actually took advantage of pulling in more open sourced data | . Candidates: When Interviewing . Don&#39;t take it personal. You may think you have work to do but data science interviews also need some fixing | Show some initiative! Start a self-project, you&#39;ll be able to show it in every interview | If you have an algorithm on your resume, you should be able to speak to it | . Interviewers: When Interviewing . Be your own worst critic. Is what you&#39;re asking the candidate to do and answer what you would expect from your current team to know? | Are you structuring interviews to allow for someone to show their creativity? Data science is not just strictly math and code | Is the interview similar to a project they would work on? | .",
            "url": "https://dpendleton22.github.io/valuebyerror/data%20science%20interviews/interviewing/2021/05/25/WaxOnWaxOff.html",
            "relUrl": "/data%20science%20interviews/interviewing/2021/05/25/WaxOnWaxOff.html",
            "date": " • May 25, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Being Basic",
            "content": "Debugging the Error: Just start coding! . Understand the basics : I was too eager to jump head first into data science initially without understanding the basics. Don&#39;t be fooled by these state-of-the-art data science blogs with beautiful code, everyone started with the basics | Every solution doesn&#39;t need to be &quot;state-of-the-art&quot;. AKA don&#39;t use the Ferrari to run errands up the street. A lot of data problems can be solved with statistics (the O.G. of data science) | Don&#39;t twiddle your thumbs over documentation. When getting introduced to a new library or tool I spent way too much time reading the documentation and watching tutorials. | . A Simple but Useful Data Science Intro . Respect and understand the basics | Be comfortable with interpreting math notation and coding the equations | Understand the importance of visualization and how to properly visualize data | When using a library try to understand what it&#39;s doing | . When initially getting into data science it can be overwhelming at first with the amount of information and models available. I can understand why someone would want to jump straight into building a deep neural network but in order to properly achieve those tasks I believe it&#39;s imperative to understand the basics and to have a sound foundation. I&#39;m all for jumping right in and coding but you&#39;ll eventually need to circle back and interpret what you coded. This is a post describing how I would introduce a friend into the data science field. I think having an understanding of some basic algorithms is a great foundation to learning new algorithms and models. . Spark Notes . At the end: Learn how to create and train a shallow neural network in PyTorch from scratch . 1. Ordinary Least Squares 2. Gradient Descent - Learning rate - Exploding Gradient - Scaling 3. PyTorch Manual Gradient Descent 4. PyTorch backpropogation and Scaling with a perceptron . TL;DR . 1. Just start coding 2. Revisit basic linear regression 3. Understand the verious ways to fit the regression model (OLS, gradient descen 3. Use that regression algorithm on a simple dataset 4. Redo regression using your neural net library of choice (TensorFlow or Pytorch) 5. Build a single perceptron model and train using backpropogation 6. Plot all your answers on the same plot to validate you got the same answers . . Copy my notes: . Check out my notes here if you want some more detail on the topics in this post: https://www.notion.so/ISL-Intro-to-Statistical-Learning-69b6f6bcc4984d438eb8d489e384bf3c . Beyond The Margins . The Dataset . The dataset is a &quot;toy set&quot; from the Introduction to Statistical Learning book by James, Witten, Hastie and Tibshirani. The book is a great introduction for those looking to have a more statistical mindset when working with models. Their book &quot;The Elements of Statistical Learning&quot; is a lot more math heavy but the two books basically cover the same topics but with a variation in depth. . The dataset is a representation of a corporation&#39;s advertising budgets and their overall sales. The various columns (TV, newspaper and radio) represent the various methods the corporation spends their advertising while the sales column is the overall amount of sales for that instance. . from __future__ import annotations import pandas as pd import numpy as np from pathlib import Path import os import matplotlib.pyplot as plt from mpl_toolkits import mplot3d import torch plt.style.use(&#39;dark_background&#39;) dataset = Path(&#39;/Users/davidpendleton/Documents/Coding/ISLR/data&#39;) advertising_ds = Path(dataset, &#39;Advertising.csv&#39;) ad_ds = pd.read_csv(advertising_ds, usecols=[&#39;TV&#39;, &#39;radio&#39;, &#39;newspaper&#39;, &#39;sales&#39;]) . . ad_ds.head(5) . TV radio newspaper sales . 0 230.1 | 37.8 | 69.2 | 22.1 | . 1 44.5 | 39.3 | 45.1 | 10.4 | . 2 17.2 | 45.9 | 69.3 | 9.3 | . 3 151.5 | 41.3 | 58.5 | 18.5 | . 4 180.8 | 10.8 | 58.4 | 12.9 | . Regression is the first step . There&#39;s no way around it, for me personally, if anyone wanted to understand data science and get into the field they need to revist and understand regression. Yes, that regression we learned in middle school, it lays the foundation for understanding some of machine learning&#39;s most used models. In this case we will start with linear regression (univariate - or one variable in this post and multivariable in another post). . Univariet Linear Regression: $y = mx + b$ . This is an equation commonly used to explain the slope and bias of a line. I personally prefer $y = beta_1 * x + beta_0$. Very commonly you&#39;ll see various books/websites use various variable representation for the same equation. It&#39;s imperative to notice these things . The task of regression is to determine the proper weight value, $ beta_1$, and bias value, $ beta_0$. This is the same goal in multivariable linear regression....and neural networks (I&#39;ll get there in a bit) but the way of getting those values is different. . Lets do the boring stuff before the fun part: OLS . Ordinary least squares (OLS) is a method to determine the weight and bias values in the linear regression problem. The goal of OLS is to determine the weight and bias values that will minimize error, this is the goal for most machine learning models. This is a common goal for a number of machine learning methods. Basically the method finds the middle of all the points to minimize the error for the fitted line. . Why are we finding the center between all the points? Because minimizing the error (or maximizing the accuracy) is the name of the game for every model you make today and in the future. Getting the center of the points, in the linear regression case, is where the error will be minimal, while still generalizing the model. . Converting math notation to code . One of the things I initially struggled with years ago when starting my masters was interpreting the equations in papers to code. I think it&#39;s a bit of a skill that of course can be developed the more comfortable you get with it. The equation we&#39;re going to work through is one of the easier ones you&#39;d see but it&#39;s a good start or reminder of how to convert a math equation into code. . The equations for OLS to solve $beta_1$ and $beta_0$ are the following: $ beta_1 = frac{ sum_{i=1}^{n}(x_i - hat{x})(y_i - hat{y})}{ sum_{i-1}^{n}(x_i - hat{x})^2}$ $ beta_0 = hat{y} - beta_1 * hat{x}$ , where $ hat{y}$ and $ hat{x}$ are the sample means (averages) of the x (input variable) and the y (target variable) . We can calculate the mean for x and y to use in the equations . y_sample = np.sum(ad_ds[&#39;sales&#39;])/len(ad_ds[&#39;sales&#39;]) x_sample = np.sum(ad_ds[&#39;TV&#39;])/len(ad_ds[&#39;TV&#39;]) y_sample, x_sample #calculating beta_1 beta_1 = np.sum((ad_ds[&#39;TV&#39;].values - x_sample)*(ad_ds[&#39;sales&#39;].values - y_sample))/ np.sum(np.square((ad_ds[&#39;TV&#39;].values - x_sample))) #calculating beta_0 beta_0 = y_sample - (beta_1*x_sample) print (f&#39;beta 0: {beta_0} nbeta 1: {beta_1}&#39;) . beta 0: 7.0325935491276965 beta 1: 0.047536640433019736 . Fit the data . We can now use the values we&#39;ve calculated for $beta_1$ and $beta_0$. We simply use vector multiplication to multiply the TV ad input data with the &quot;slope&quot; or $ beta_1$ and add the bias, $ beta_0$ . #beta 1 is the slope y_pred = beta_0 + beta_1 * ad_ds[&#39;TV&#39;] . plt.title(&quot;Linear Regression: OLS&quot;) plt.scatter(ad_ds[&#39;TV&#39;], ad_ds[&#39;sales&#39;], color=&#39;red&#39;, label=&#39;raw sales outcomes&#39;) plt.plot(ad_ds[&#39;TV&#39;], y_pred, label=&#39;predcited sales outcomes (OLS)&#39;) plt.xlabel(&#39;TV&#39;) plt.ylabel(&#39;Sales&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7febfd1789d0&gt; . We can measure the error of our fitted line by measuring the residual sum of squares (RSS). The RSS does not have a predefined range and is dependent on the scale on the y value. . RSS = np.sum(np.square(ad_ds[&#39;sales&#39;] - (beta_0 + beta_1*ad_ds[&#39;TV&#39;]))) . RSS . 2102.5305831313512 . We can give a little better context to the fit of the line by also calculating the $R^2$ value The $R^2$ value is a fraction of the model variance over the dependant variable variance. It is not a metric to conclude how well our model fits the data . TSS = np.sum(np.square(ad_ds[&#39;sales&#39;] - ad_ds[&#39;sales&#39;].mean())) . r_squared = 1 - (RSS/TSS) . r_squared . 0.611875050850071 . The collapsed cell gives a brief overview how we can use plots to validate we&#39;ve found the best weight and bias value. The following plot displays a sweep of the slope values and the RSS values for those slope values. The &#39;red dot&#39; represents the slope values our least squares equation determined and the RSS value which we can see is the minimum. Plotting is very imperative in data science and everyone has their preference but I show a few different ways to show the sweep of values . Using PyTorch . This might be over kill but having a complete understanding of regression is a great introduction to understanding neural networks (in my opinion). Coupled with my desire to make PyTorch my new primary library for neural network models, I want to try and complete these models with PyTorch tensors. . This is a brief method I usually use when learning a new language or library. I complete a simple script where I know the expected output. This gives me a good insight to the variations in libraries and how I can produce the same output. I want to repeat the fit of the data with PyTorch . The data fit is completed in three various methods: . OLS with PyTorch Tensors | Manual gradient descent with PyTorch | Using PyTorch&#39;s gradient descent | Using a perceptron in PyTorch | Ordinary Least Squares with PyTorch Tensors | This is a very straight forward approach. The equations are the same as the OLS example above but with PyTorch tensors . #lets read in these values as tensors from the dataframe torch_tensor = torch.tensor(ad_ds[[&#39;TV&#39;, &#39;sales&#39;]].values) #tensors can also be read in from numpy arrays torch_tensor print (f&quot;The tensor shape is: {torch_tensor.shape}&quot;) #lets calculate the sample mean for the two variables x_samp = torch_tensor[:, 0].mean() y_samp = torch_tensor[:, 1].mean() beta_1_least = torch.sum((torch_tensor[:, 0] - x_samp)*(torch_tensor[:, 1] - y_samp)) / torch.sum(((torch_tensor[:, 0] - x_samp)**2)) beta_0_least = (y_samp - (beta_1 * x_samp)) y_pred = torch_tensor[:, 0] * beta_1 + beta_0 plt.plot(torch_tensor[:, 0], y_pred, label=&#39;predcited sales outcomes (OLS)&#39;) plt.scatter(torch_tensor[:, 0], torch_tensor[:, 1], c=&#39;red&#39;, label=&#39;raw sales outcomes&#39;) plt.xlabel(&#39;TV&#39;) plt.ylabel(&#39;Sales&#39;) plt.legend() . . The tensor shape is: torch.Size([200, 2]) . &lt;matplotlib.legend.Legend at 0x7febfd5cbed0&gt; . Gradient Descent . Instead of using the least squares method, lets look at the gradient descent method with PyTorch. Before we can jump into working with PyTorch for a model, its important to understand how it reaches the same answer as an OLS method when using gradient descent. Having a good understanding of gradient descent and how it works by hand will help you down the road when working with other backpropogation methods such as Adam, stoichastic gradient descent and others. . Gradient descent is a weight adjustment method that uses a cost function and a learning rate to iteratively update the weights to minimize error. The cost function is the measure of error, this is what we are trying to minimize. We can use the derivative of the cost function to update our weights because the derivate provides the direction of minimum error. . The cost function is $Cost = sum_{i=1}^{n}(y_i - prediction)^2$ and its derivative is calculated using a partial derivative for the variables of interest which would be $ beta_1$ and $ beta_0$. The partial derivatives are solved using the chain and product rule from calculus to get the following equations: $ frac{ partial{Cost}}{ partial{ beta_0}} = 2 * (y_i - prediction)$ $ frac{ partial{Cost}}{ partial{ beta_1}} = 2 * (y_i - prediction) * x, $ The notes give some insight on the equation and the importance of the learning weight. The learning rate is used to control how fast we update the beta variables. . Peak at my notes to see some more detail on gradient descent and how the learning rate affects the calculation of the weights: https://www.notion.so/ISL-Intro-to-Statistical-Learning-69b6f6bcc4984d438eb8d489e384bf3c . def gradient_descent(lr:float, x:torch.Tensor, y:torch.tensor, beta_0:float, beta_1:float) -&gt; Tuple(float, float): &quot;&quot;&quot;Run the gradient descent calculation Parameters - lr: float The learning rate for the calculation update x: torch.tensor A vector of tensors of the input values y: torch.tensor A vector of tensors of the out variable beta_0: float The beta_0 bias beta_1: float The beta_1 weight Returns - Tuple(float, float) The bias, beta_0, and beta weight, beta_1, are returned &quot;&quot;&quot; predictions = beta_1 * x + beta_0 error = y - predictions beta_1 += torch.mean(error * x) * lr beta_0 += torch.mean(error) * lr return beta_0, beta_1 . Controlling the rate of learning . Learning rate is a paramter that is commonly explored and analyzed when developing a model. It&#39;s an important paramter in the model training phase because it not only helps control the updates of the calculations, it controls how fast your model learns. Set this learning rate too low and it will take a long time to converge (and likely to get stuck in some local minima), set the value too high and the gradient will explode! I&#39;ll show you what I mean in this section . Exploding Gradient . This happens when the learning rate is set too high and the weights are basically bouncing all over the place until they get too large to provide any significant contribution . beta_1 = torch.rand(1) beta_0 = torch.rand(1) beta_0_hist = [] beta_1_hist = [] # if the learning rate is too large the gradient explodes #this is likely because the data is not scaled so large swings in the gradient are observed #set the learning rate to 0.001, 1e-3 and 1e-2 are common starting points for me lr = 1e-3 #run the gradient descent for 50 epochs #An epoch is the definition for a full update of all the data, we can look into batch updates later for x in range(0, 50): beta_0, beta_1 = gradient_descent(lr, torch_tensor[:, 0], torch_tensor[:, 1], beta_0, beta_1) # print (beta_0, beta_1); beta_0_hist.append(float(beta_0)) beta_1_hist.append(float(beta_1)) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5)) ax1.plot(beta_0_hist) ax1.title.set_text(&#39;Beta 0 gradient history&#39;) ax1.set_ylabel(&#39;Beta value&#39;) ax1.set_xlabel(&#39;Epoch&#39;) ax2.plot(beta_1_hist) ax2.title.set_text(&#39;Beta 1 gradient history&#39;) ax2.set_ylabel(&#39;Beta value&#39;) ax2.set_xlabel(&#39;Epoch&#39;) . Text(0.5, 0, &#39;Epoch&#39;) . The beta values get so large we start procuding nan values...something is definitely wrong with the current gradient descent method . Lets decrease the learning rate and see what happens... . beta_1 = torch.rand(1) beta_0 = torch.rand(1) beta_0_hist = [] beta_1_hist = [] # if the learning rate is too small the gradient takes forever to update #set the learning rate to 1e-5 lr = 2e-5 #run the gradient descent for 50 epochs #An epoch is the definition for a full update of all the data, we can look into batch updates later for x in range(0, 1000): beta_0, beta_1 = gradient_descent(lr, torch_tensor[:, 0], torch_tensor[:, 1], beta_0, beta_1) # print (beta_0, beta_1); beta_0_hist.append(float(beta_0)) beta_1_hist.append(float(beta_1)) . The two plots below show the gradient descent progression for beta_0 and beta_1 with a learning rate of 1e-5 for 1000 epochs. In certain situations these parameters are fine but with the simplicity of this data it should not be this complex to determine the beta values. As we can see in the plots below, the two parameters are trending towards the values we know they need to converge to. There is a quicker way to converge these values which is imperative and a common preprocessing step for neural networks, SCALING. Oh and I actually did run this gradient descent just to see how long it would take to converge and its on the order of a million plus epochs (this is a no no for this simple of a problem!) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5)) ax1.plot(beta_0_hist) ax1.title.set_text(&#39;Beta 0 gradient history&#39;) ax1.set_ylabel(&#39;Beta value&#39;) ax1.set_xlabel(&#39;Epoch&#39;) ax2.plot(beta_1_hist) ax2.title.set_text(&#39;Beta 1 gradient history&#39;) ax2.set_ylim((0.078, 0.08)) ax2.set_ylabel(&#39;Beta value&#39;) ax2.set_xlabel(&#39;Epoch&#39;) . Text(0.5, 0, &#39;Epoch&#39;) . Scaling . The reason for gradient descent being so sensitive to the learning rate we set was because the values to update the parameters were too sparse which caused a lot of variation in our values. This caused our gradient to explode. When setting the value too small, we were moving at a snails pace to actually converge to the global minimum values. We can solve this issue by making all our values used during gradient descent to get cozy and closer together so we aren&#39;t seeing large values when updating our beta values . I&#39;m using a minmax scaling method where I set all the values between 0 and 1 using the minimum and maximum values of each variable . torch_tv_scaled = (torch_tensor[:, 0] - torch_tensor[:, 0].min()) / (torch_tensor[:, 0].max() - torch_tensor[:, 0].min()) torch_sales_scaled = (torch_tensor[:, 1] - torch_tensor[:, 1].min()) / (torch_tensor[:, 1].max() - torch_tensor[:, 1].min()) . As you can see in the plots below the relationship between the independent and dependent variables are maintained, the data is simply scaled between 0 and 1. This will allow the gradient descent to train faster and not explode with a larger learning rate . It should also be reminded and noted, our OLS method determined the optimal value for beta_1 and beta_0 were 0.0475 and 7.0326. Since we scaled our dataset those values will not be what the scaled gradient descent will find. . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5)) ax1.scatter(torch_tensor[:, 0], torch_tensor[:, 1], c=&#39;red&#39;) ax1.title.set_text(&#39;TV Ads vs Sales&#39;) ax1.set_ylabel(&#39;Sales&#39;) ax1.set_xlabel(&#39;TV Advertisement&#39;) ax2.scatter(torch_tv_scaled, torch_sales_scaled, c=&#39;red&#39;) ax2.title.set_text(&#39;TV Ads vs Sales Min Max Scaled&#39;) ax2.set_ylabel(&#39;Sales&#39;) ax2.set_xlabel(&#39;TV Advertisement&#39;) . Text(0.5, 0, &#39;TV Advertisement&#39;) . beta_1_scaled = torch.rand(1) beta_0_scaled = torch.rand(1) beta_0_hist = [] beta_1_hist = [] # learning rate is very slow and finds a local min #this is likely because the data is not scaled so large swings in the gradient are observed lr = 0.1 for x in range(0, 1000): beta_0_scaled, beta_1_scaled = gradient_descent(lr, torch_tv_scaled, torch_sales_scaled, beta_0_scaled, beta_1_scaled) # print (beta_0_scaled, beta_1_scaled); beta_0_hist.append(float(beta_0_scaled)) beta_1_hist.append(float(beta_1_scaled)) . We can clearly see using a relatively larger learning rate, 0.1, and 1000 epochs we are able to see the beta_0 and beta_1 values converge to their values of 0.2153 and 0.5532 for the scaled dataset. The plot on the far right shows those converged values mapped to our scaled dataset, which is the same trend as the OLS solution . fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5)) ax1.plot(beta_0_hist) ax1.title.set_text(&#39;Beta 0 scaled gradient history&#39;) ax1.set_ylabel(&#39;Beta value&#39;) ax1.set_xlabel(&#39;Epoch&#39;) ax2.plot(beta_1_hist) ax2.title.set_text(&#39;Beta 1 scaled gradient history&#39;) ax2.set_ylabel(&#39;Beta value&#39;) ax2.set_xlabel(&#39;Epoch&#39;) ax3.title.set_text(&#39;Scaled TV Ads vs Scaled Sales&#39;) ax3.set_ylabel(&#39;Scaled Sales&#39;) ax3.set_xlabel(&#39;Scaled TV Ads&#39;) ax3.plot(torch_tv_scaled, torch_tv_scaled * beta_1_scaled + beta_0_scaled, c=&#39;green&#39;) ax3.scatter(torch_tv_scaled, torch_sales_scaled, c=&#39;red&#39;) . &lt;matplotlib.collections.PathCollection at 0x7febfdcc7090&gt; . beta_0_scaled, beta_1_scaled . (tensor([0.2150]), tensor([0.5537])) . If you&#39;d like to get your dataset and predictions to their original scaling don&#39;t trip we can simply inverse the minmax equation we used to reset our dataset and our predicted values. Use the following inverse minmax equation: $y_{pred} = (y_{scaled pred} * (max(sales) - min(sales))) + min(sales)$ . y_scaled_pred = (torch_tv_scaled * beta_1_scaled + beta_0_scaled)*(torch_tensor[:, 1].max() - torch_tensor[:, 1].min()) + torch_tensor[:, 1].min() . y_pred_least = torch_tensor[:, 0] * beta_1_least + beta_0_least . PRESTO! The scaled dataset is back to it&#39;s original form. The plot below also shows the OLS original fitted plot, can you see the difference....me neither. The two methods give the same outcome because they are both trying to minimze the error. The only difference is we can use this knowledge of gradient descent with neural networks! . plt.plot(torch_tensor[:, 0], y_pred_least, label=&#39;y_pred_gradient&#39;) plt.plot(torch_tensor[:, 0], y_scaled_pred, c=&#39;green&#39;, label = &quot;y_pred_OLS&quot;) plt.scatter(torch_tensor[:, 0], torch_tensor[:, 1], c=&#39;red&#39;) plt.xlabel(&#39;TV Ads&#39;) plt.ylabel(&#39;Sales&#39;) plt.title(&#39;TV Ads vs. Sales&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7febfe3acf90&gt; . This gradient descent wasn&#39;t discussed in the Stats Learning book in reading but I thought the dots can be connected to show why imperative preprocessing such as scaling are used and how weights are updated using vectors. When someone learns these two things they can likely wrap their mind around the &quot;magic&quot; of a neural network . Backpropagation . Below we go over how to use PyTorch&#39;s backpropagation for calculating the weight and bias for our sales data. I include some notes describing the code, feel free to peak at my notes (I won&#39;t tell) . criterion = torch.nn.MSELoss() weight = torch.rand(1, dtype=torch.double).requires_grad_() bias = torch.rand(1, dtype=torch.double, requires_grad=True) def model(x): return x * weight + bias lr = 2e-5 for epoch in range(100): data = torch_tensor[:, 0] outputs = model(data) loss = criterion(outputs, torch_tensor[:, 1]) loss.backward() with torch.no_grad(): weight -= weight.grad * lr bias -= bias.grad * lr weight.grad.zero_() bias.grad.zero_() . Margin Notes . . criterion(model(torch_tensor[0, 0].unsqueeze(dim=0)), torch_tensor[0, 1]); model(torch_tensor[0, 0].unsqueeze(dim=0)); torch_tensor[0, 1] weight, bias . . (tensor([0.0822], dtype=torch.float64, requires_grad=True), tensor([0.2079], dtype=torch.float64, requires_grad=True)) . Scaling the dataset . weight = torch.rand(1, dtype=torch.double).requires_grad_() bias = torch.rand(1, dtype=torch.double, requires_grad=True) def model(x): return x * weight + bias #all the margin notes above apply to this code lr = .1 for epoch in range(1000): #using our min max scaled dataset data = torch_tv_scaled outputs = model(data) loss = criterion(outputs, torch_sales_scaled) loss.backward() #the optimizer in pyTorch replaces this portion of code with torch.no_grad(): weight -= weight.grad * lr bias -= bias.grad * lr weight.grad.zero_() bias.grad.zero_() pred = model(torch_tv_scaled.unsqueeze(dim=0))*(torch_tensor[:, 1].max() - torch_tensor[:, 1].min()) + torch_tensor[:, 1].min() . . Using a perceptron . A neural network is comprised of perceptrons. The goal of training a neural network is to determine the weight and bias values for each individual perceptron. PyTorch allows a lot of the backpropagation to be handled with fewer lines of code. The example below reaches the same calculated values but with a defined perceptron and using PyTorch&#39;s optimizer code to backpropagate the perceptron for us. It&#39;s easy to update the model when it&#39;s a single perceptron but these tools come in handy when using a large neural network with hundreds of perceptrons. . import torch.nn as nn import torch.nn.functional as F import torch.optim as optim class perceptron(nn.Module): def __init__(self): super(perceptron, self).__init__() self.percep = nn.Linear(1, 1) def forward(self, data): return self.percep(data) percept = perceptron() . percept = percept.float() . optimizer = optim.SGD(percept.parameters(), lr=1e-3) hist_loss = [] for i in range(0, 50000): optimizer.zero_grad() pred = percept(torch_tv_scaled.float().unsqueeze(-1)) loss = criterion(pred, torch_sales_scaled.float().unsqueeze(-1)) hist_loss.append(loss.item()) loss.backward() optimizer.step() . plt.title(&quot;Loss over epochs&quot;) plt.plot(hist_loss) . [&lt;matplotlib.lines.Line2D at 0x7fec01c59cd0&gt;] . preds = percept(torch_tv_scaled.float().unsqueeze(-1))*(torch_tensor[:, 1].max() - torch_tensor[:, 1].min()) + torch_tensor[:, 1].min() . preds = preds.detach().numpy() . preds = preds.flatten() . All calculated outputs . plt.scatter(torch_tensor[:, 0], torch_tensor[:, 1], c=&#39;red&#39;) plt.plot(torch_tensor[:, 0], y_pred, label=&#39;Gradient Descent&#39;) plt.plot(torch_tensor[:, 0], y_pred_least, c=&#39;green&#39;, label=&#39;OLS&#39;) plt.plot(torch_tensor[:, 0], preds, label=&#39;NN: 20k epochs&#39;) plt.legend() plt.savefig(&#39;all-regression-plots.png&#39;) . You can now use that perceptron to make predictions of how much revenue certain ads will bring the team! .",
            "url": "https://dpendleton22.github.io/valuebyerror/pytorch/learning%20rate/exploding%20gradient/linear%20regression/gradient%20descent/ordinary%20least%20squares/2021/04/29/being_basic.html",
            "relUrl": "/pytorch/learning%20rate/exploding%20gradient/linear%20regression/gradient%20descent/ordinary%20least%20squares/2021/04/29/being_basic.html",
            "date": " • Apr 29, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "The McFly Effect: Logitic Regression",
            "content": "The McFly Effect . When someone gets into data science there are usually two things they&#39;d like to predict. Sports game outcomes and the stock market...and I&#39;m no different. One of the first projects I did in data science (2 years ago) was predicting the outcome of NBA games . . Using a pipeline . We can use the pipeline created in the last post to preprocess our data. Using pipelines is an efficient way to ensure there is no data leakage when developing a model. Data leakage is when data that normally would not be accessible to the model during usage is present in the training set. This can be the column or interest or an extra column that is not always available. . . def data_pipeline(df): test = df.columns.map(&#39;.&#39;.join).str.strip(&#39;.&#39;) df.columns = test.str.replace(r&quot;Unnamed: [0-9]_level_[0-9].&quot;, &#39;&#39;, regex=True) df[&#39;playing_home&#39;] = df[&#39;Unnamed: 3_level_1&#39;].isnull() df.drop(columns=[&#39;Unnamed: 3_level_1&#39;, &#39;Unnamed: 24_level_0.Unnamed: 24_level_1&#39;], inplace=True) df[&#39;dub&#39;] = df[&#39;W/L&#39;] == &#39;W&#39; df.drop(columns=[&#39;W/L&#39;], inplace=True) return df . sac_2017_df = pd.read_csv(sac_2017_szn, header=[0, 1]).pipe(data_pipeline) sac_2018_df = pd.read_csv(sac_2018_szn, header=[0, 1]).pipe(data_pipeline) mil_2017_df = pd.read_csv(mil_2017_szn, header=[0, 1]).pipe(data_pipeline) mil_2018_df = pd.read_csv(mil_2018_szn, header=[0, 1]).pipe(data_pipeline) . Why are we looking at this NBA data from 2017 my guy? . Good question, which I have a few answers to. First, this is the earliest data science project I can remember doing outside of my master&#39;s class. In addition, when working with time series data it can be beneficial to work out your model with historic data. If the model doesn&#39;t work well on previous events we can be pretty sure it will have the same performance on more up to date data. Lastly, I presented it during my job interview which I currently have today. . For this specific post we will only work with the Sacramento Kings&#39; schedule . sac_szn = pd.concat([sac_2017_df, sac_2018_df]) . The main objective is to determine if a team is going to win the game of interest so let&#39;s remove that from the dataset and make it its own labeled series. This is apart of that data leakage conflict I wrote about earlier. We want to avoid the model from seeing the actual game outcomes . In addition to the win/loss column which I named &quot;dub&quot;. I dropped the &#39;playing home&#39; column which teams me if the Kings are playing home or away that game. This is a boolean indicator which is predetermined before the game so instead of averaging this value, I can use it&#39;s original value to infer if the Kings have home court advantage. . sac_szn_home = sac_szn[&#39;playing_home&#39;] . sac_szn_dub = sac_szn[&#39;dub&#39;] . sac_szn.drop(columns=[&#39;Rk&#39;, &#39;G&#39;, &#39;Date&#39;, &#39;Opp&#39;, &#39;Tm&#39;, &#39;Opp&#39;, &#39;dub&#39;, &#39;playing_home&#39;], inplace=True) . In order to predict the outcome of a game we of course can&#39;t look at the actual stats for that game and use them as an input. We are trying to predict the outcome before it happens so that includes the stats for the game. Instead we need to use some prior knowledge from previous games to make our prediction. I remember deciding to use a 3 day rolling average from both stat lines. This makes a lot of assumptions which I&#39;ll explain later in this post . Since the dataset is already a pandas dataframe we can just create the rolling average using the &quot;.rolling()&quot; function. This takes the length of time to roll as input which is set to 3. This is because each row represents a game. The &quot;.mean()&quot; function takes the average across the 3 games as the new value for the rolling results. This can also be used to create a rolling sum, rolling difference, etc. . sac_szn_rolling = sac_szn.rolling(3).mean() . sac_szn_rolling[&#39;playing_home&#39;] = sac_szn_home . sac_szn_rolling[&#39;dub&#39;] = sac_szn_dub . After taking the rolling average, the game outcome needs to be added back in and the indicator of playing at home or away. In addition the outcome of the game needs to be shifted. This shift removes the stats for the game of interest from being used as an input for the prediction. . sac_szn_rolling[[&#39;playing_home&#39;, &#39;dub&#39;]] = sac_szn_rolling[[&#39;playing_home&#39;, &#39;dub&#39;]].shift(periods=-1) . sac_szn_rolling[[&#39;playing_home&#39;, &#39;dub&#39;]] = sac_szn_rolling[[&#39;playing_home&#39;, &#39;dub&#39;]].astype(float) . We can remove the edges of the dataframe as these rows do not have information from the rolling average and game outcome shift . sac_szn_rolling = sac_szn_rolling.iloc[2:-1, :] . Train / Validation Split . I&#39;ll get more into details in more posts about best practices but we split this data into two segments, training and validation splits. The training set is used to develop the model, the model makes its adjustments with this dataset. The validation set is solely used to check the outcome of training the model. This portion of the data is never used to train the model. . For this specific dataset because it is a schedule of games, I decided to train on the first 80% of games and validate on the later 20%. In other cases, the data can be shuffled then split but that can be explored later . split = int(len(sac_szn_rolling)*0.8) . sac_szn_train, sac_szn_train_label = sac_szn_rolling.iloc[:split, :], sac_szn_rolling[&#39;dub&#39;][:split] . sac_szn_test, sac_szn_test_label = sac_szn_rolling.iloc[split:, :], sac_szn_rolling[&#39;dub&#39;][split:] . %%capture sac_szn_train.drop(columns=[&#39;dub&#39;], inplace=True) sac_szn_test.drop(columns=[&#39;dub&#39;], inplace=True) . When working with data preprocess and data splitting, it&#39;s a good idea to check that no rows of information are lost during the process. In this case, the original number of rows should equal the sum of the training and test split. . len(sac_szn_rolling), len(sac_szn_train) + len(sac_szn_test) . (161, 161) . Logistic Regression and How it works . What is Logistic Regression or Logit . Logistic regression is a regression model commonly used for classification which calculates the probability of an outcome using parameter weights on the input data. This calculated probability is used against an activation function, sigmoid, to give a predicted classification output | . How is the algorithm used?? . This algorithm is commonly used for classification of discreet values. It can be used for multivariate classification but there is risk the parameter weights that are calculated will be unstable if each of the classification types are significantly different in distributions | . Ok, so how does it actually work? . The equation logistic regression uses to calculate the probability of the input values being 0 or 1 is the following: . $P(X) = frac{e^{ beta_0 + beta_p * X}}{1 + e^{ beta_0 + beta_p * X}}$ . where the beta variables are the weights coefficients which are multiplied with the input values, X. The number of beta values is equal to the number of input X values. . For binary classification - Parameter weights which match the length of independent variables are adjusted using a loss function and gradient descent. The probability of the input variables with the parameter weights equaling the value of 1 is calculated with the parameter weights. That probability is passed through the sigmoid activation function to get a binary classification prediction. Then the loss is calculated using gradient descent. . | Logistic regression could use least squares ,like linear regression, to determine the parameters but instead maximum likelihood is used. Maximum likelihood attempts to find values for the parameters in a way that the probability of all individuals that fit a specific classification would be 1 and 0 for the others that are not in that class. . | . Do input data need to be scaled? . No the values in the model do not need to be scaled | As long as the data used to make a prediction is on the same scale as the data used to optimize the parameter values | . We can use a logistic regression model library to train and test the model. The first library we can use is the logistic regression model from sklearn . clf = LogisticRegressionCV(solver=&#39;liblinear&#39;, max_iter=200).fit(sac_szn_train, sac_szn_train_label) . print (f&quot;The current number of features are: {len(clf.coef_[0])}&quot;) . The current number of features are: 33 . Class balance . When working with a classification model, be sure to check the classification representation from each label. Imbalance within a dataset can affect the training of a model. The current datatset we have is imbalanced towards 0 or losses for the Kings. We can address this later if unsatisfactory model results occur. . . for i in sac_szn_train_label.unique(): print (f&#39;Count for {i}: {len(sac_szn_train_label.loc[sac_szn_train_label == i])}&#39;) . Count for 0.0: 77 Count for 1.0: 51 . Initial model results: 60% looks better than a coin flip . clf.score(sac_szn_train, sac_szn_train_label) . 0.6015625 . The learning curve is your friend . The learning curve gives detail on the progress made during model training. It can be used to determine underfits and overfits of your model . %%capture train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(clf, sac_szn_train, sac_szn_train_label, train_sizes=np.linspace(0.5, 1.0, 10), return_times=True, verbose=10); . plt.title(&quot;Logistic Regression: Learning Curve&quot;) plt.plot(train_sizes, train_scores.mean(axis=1), label=&#39;training&#39;) plt.plot(train_sizes, test_scores.mean(axis=1), label=&#39;validation&#39;) plt.ylabel(&quot;Accuracy (%)&quot;) plt.xlabel(&quot;Epochs&quot;) plt.grid() plt.legend() . &lt;matplotlib.legend.Legend at 0x7fa39f4cda90&gt; . The overall shape of the curve makes sense but there appears to be some noise or something interesting with the data from the jump in accuracy. We can have a better look at how the algorithm is predicting with a confusion matrix . Confusion Matrix . The confusion matrix is a great visual tool for observing classification performance of a model. It includes a matrix showing the true positives, true negatives, false positives and false negatives of the algorithm on the data . Enter the Matrix: . cm = confusion_matrix(sac_szn_train_label, clf.predict(sac_szn_train)) fig, ax = plt.subplots(figsize=(8, 8)) ax.imshow(cm) ax.grid(False) ax.xaxis.set(ticks=(0, 1), ticklabels=(&#39;Predicted 0s&#39;, &#39;Predicted 1s&#39;)) ax.yaxis.set(ticks=(0, 1), ticklabels=(&#39;Actual 0s&#39;, &#39;Actual 1s&#39;)) ax.set_ylim(1.5, -0.5) for i in range(2): for j in range(2): ax.text(j, i, cm[i, j], ha=&#39;center&#39;, va=&#39;center&#39;, color=&#39;red&#39;) plt.show() . 60% doesn&#39;t look so good now... . This confusion matrix shows our model can easily get to 60% accuracy by simply predicting 0 or &#39;loss&#39; for the team which is not how we want our model to predict. The goal of the model is to generalize the data . . Generalizing the logistic regression algorithm . From the confusion matrix we can see the algorithm is simply predicting 0 (loss) every time to get a 60% accuracy score. We can tune the logistic regression to regularize the algorithm to make better predictions . clf = LogisticRegressionCV(solver=&#39;liblinear&#39;, Cs=1000, max_iter=200).fit(sac_szn_train, sac_szn_train_label) . clf.score(sac_szn_train, sac_szn_train_label) . 0.6015625 . %%capture train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(clf, sac_szn_train, sac_szn_train_label, train_sizes=np.linspace(0.5, 1.0, 10), return_times=True, verbose=10) . plt.title(&quot;Logistic Regression: Learning Curve&quot;) plt.plot(train_sizes, train_scores.mean(axis=1), label=&quot;training&quot;) plt.plot(train_sizes, test_scores.mean(axis=1), label=&#39;validation&#39;) plt.ylabel(&quot;Accuracy (%)&quot;) plt.xlabel(&quot;Epochs&quot;) plt.grid() plt.legend() . &lt;matplotlib.legend.Legend at 0x7fa39f073810&gt; . cm = confusion_matrix(sac_szn_train_label, clf.predict(sac_szn_train)) fig, ax = plt.subplots(figsize=(8, 8)) ax.imshow(cm) ax.grid(False) ax.xaxis.set(ticks=(0, 1), ticklabels=(&#39;Predicted 0s&#39;, &#39;Predicted 1s&#39;)) ax.yaxis.set(ticks=(0, 1), ticklabels=(&#39;Actual 0s&#39;, &#39;Actual 1s&#39;)) ax.set_ylim(1.5, -0.5) for i in range(2): for j in range(2): ax.text(j, i, cm[i, j], ha=&#39;center&#39;, va=&#39;center&#39;, color=&#39;red&#39;) plt.show() . Attempts to regularize the logistics regression in sklearn are showing no significant difference from the first results. Time to introduce StatsModels a great statistical library to use . Stats models logistic regression library . log_reg = sm.Logit(sac_szn_train_label, sac_szn_train).fit() . Optimization terminated successfully. Current function value: 0.551797 Iterations 6 . print (log_reg.summary()) . Logit Regression Results ============================================================================== Dep. Variable: dub No. Observations: 128 Model: Logit Df Residuals: 95 Method: MLE Df Model: 32 Date: Sat, 23 Jan 2021 Pseudo R-squ.: 0.1793 Time: 00:02:35 Log-Likelihood: -70.630 converged: True LL-Null: -86.064 Covariance Type: nonrobust LLR p-value: 0.5237 ================================================================================ coef std err z P&gt;|z| [0.025 0.975] -- Team.FG -0.4993 0.995 -0.502 0.616 -2.449 1.450 Team.FGA -0.0249 0.447 -0.056 0.956 -0.901 0.851 Team.FG% 10.2459 88.084 0.116 0.907 -162.396 182.887 Team.3P -0.0556 1.014 -0.055 0.956 -2.044 1.933 Team.3PA 0.0612 0.381 0.161 0.872 -0.686 0.808 Team.3P% -1.0472 23.788 -0.044 0.965 -47.671 45.576 Team.FT -0.0420 0.892 -0.047 0.962 -1.790 1.706 Team.FTA -0.1774 0.641 -0.277 0.782 -1.433 1.078 Team.FT% -3.9722 13.852 -0.287 0.774 -31.121 23.177 Team.ORB 0.1043 0.365 0.286 0.775 -0.610 0.819 Team.TRB -0.0082 0.233 -0.035 0.972 -0.464 0.448 Team.AST 0.0171 0.153 0.112 0.911 -0.282 0.316 Team.STL -0.0892 0.233 -0.382 0.702 -0.547 0.368 Team.BLK 0.4874 0.283 1.724 0.085 -0.067 1.041 Team.TOV -0.0883 0.326 -0.271 0.787 -0.728 0.551 Team.PF -0.1124 0.195 -0.576 0.564 -0.495 0.270 Opponent.FG 0.7371 0.928 0.794 0.427 -1.082 2.556 Opponent.FGA 0.1074 0.464 0.232 0.817 -0.801 1.016 Opponent.FG% -46.9855 76.885 -0.611 0.541 -197.678 103.707 Opponent.3P 0.8309 1.073 0.774 0.439 -1.272 2.934 Opponent.3PA -0.2885 0.420 -0.687 0.492 -1.111 0.534 Opponent.3P% -12.1495 33.259 -0.365 0.715 -77.337 53.038 Opponent.FT -0.5887 0.718 -0.820 0.412 -1.996 0.819 Opponent.FTA 0.7238 0.564 1.283 0.200 -0.382 1.830 Opponent.FT% 19.3377 16.380 1.181 0.238 -12.766 51.442 Opponent.ORB -0.1427 0.361 -0.396 0.692 -0.850 0.564 Opponent.TRB -0.3266 0.254 -1.286 0.199 -0.824 0.171 Opponent.AST -0.0918 0.149 -0.618 0.537 -0.383 0.200 Opponent.STL 0.0580 0.325 0.178 0.858 -0.579 0.695 Opponent.BLK 0.1431 0.212 0.674 0.500 -0.273 0.559 Opponent.TOV 0.3809 0.237 1.609 0.108 -0.083 0.845 Opponent.PF 0.1081 0.222 0.487 0.626 -0.327 0.543 playing_home 0.8333 0.511 1.632 0.103 -0.168 1.834 ================================================================================ . The current coefficient values in the output value most Team.FG%, Team.FT%, Opp.FG%, Opp.3P% and Opp.FT% . This is based on coefficient values and their contribution to the calculated probability. Observing the z-statistical values none of them are large and none of the p values less than 0.01. This means we can not conclude there is a clear significance between these values and the team winning. Results such as these give me more insights to adding more data or revisiting the preprocessing method. . pred = log_reg.predict(sac_szn_train) . accuracy_score(sac_szn_train_label, list(map(round, pred))) . 0.734375 . This confusion matrix looks a lot more dispersed on predictions which is the behavior we want to see in the model . cm = confusion_matrix(sac_szn_train_label, list(map(round, pred))) fig, ax = plt.subplots(figsize=(8, 8)) ax.imshow(cm) ax.grid(False) ax.xaxis.set(ticks=(0, 1), ticklabels=(&#39;Predicted 0s&#39;, &#39;Predicted 1s&#39;)) ax.yaxis.set(ticks=(0, 1), ticklabels=(&#39;Actual 0s&#39;, &#39;Actual 1s&#39;)) ax.set_ylim(1.5, -0.5) for i in range(2): for j in range(2): ax.text(j, i, cm[i, j], ha=&#39;center&#39;, va=&#39;center&#39;, color=&#39;red&#39;) plt.show() . sac_szn_train_down = sac_szn_train[[&#39;playing_home&#39;, &#39;Opponent.TOV&#39;, &#39;Opponent.FT%&#39;, &#39;Opponent.FTA&#39;, &#39;Team.BLK&#39;, &#39;Opponent.TRB&#39;]] . log_reg_second = sm.Logit(sac_szn_train_label, sac_szn_train_down).fit() . Optimization terminated successfully. Current function value: 0.657341 Iterations 5 . print (log_reg_second.summary()) . Logit Regression Results ============================================================================== Dep. Variable: dub No. Observations: 128 Model: Logit Df Residuals: 122 Method: MLE Df Model: 5 Date: Thu, 21 Jan 2021 Pseudo R-squ.: 0.02236 Time: 21:35:28 Log-Likelihood: -84.140 converged: True LL-Null: -86.064 Covariance Type: nonrobust LLR p-value: 0.5715 ================================================================================ coef std err z P&gt;|z| [0.025 0.975] -- playing_home 0.3219 0.370 0.870 0.384 -0.403 1.047 Opponent.TOV 0.0465 0.076 0.611 0.541 -0.103 0.196 Opponent.FT% 0.2939 2.150 0.137 0.891 -3.921 4.508 Opponent.FTA -0.0244 0.048 -0.505 0.614 -0.119 0.070 Team.BLK 0.2379 0.162 1.468 0.142 -0.080 0.556 Opponent.TRB -0.0428 0.044 -0.971 0.332 -0.129 0.044 ================================================================================ . pred = log_reg_second.predict(sac_szn_test[[&#39;playing_home&#39;, &#39;Opponent.TOV&#39;, &#39;Opponent.FT%&#39;, &#39;Opponent.FTA&#39;, &#39;Team.BLK&#39;, &#39;Opponent.TRB&#39;]]) . accuracy_score(sac_szn_test_label, list(map(round, pred))) . 0.5454545454545454 . cm = confusion_matrix(sac_szn_test_label, list(map(round, pred))) fig, ax = plt.subplots(figsize=(8, 8)) ax.imshow(cm) ax.grid(False) ax.xaxis.set(ticks=(0, 1), ticklabels=(&#39;Predicted 0s&#39;, &#39;Predicted 1s&#39;)) ax.yaxis.set(ticks=(0, 1), ticklabels=(&#39;Actual 0s&#39;, &#39;Actual 1s&#39;)) ax.set_ylim(1.5, -0.5) for i in range(2): for j in range(2): ax.text(j, i, cm[i, j], ha=&#39;center&#39;, va=&#39;center&#39;, color=&#39;red&#39;) plt.show() . Next Steps: Lets LEVEL UP! . As within any data science project, there are a number of next steps or changes that can be added to a project to level up. For this specific project I believe adding more descriptive information about the data and each team would assist the logistic regression model. To be more specific, there are advanced analytics used in the world of sports analytics that would help this project. For example, there is an ELO rating . . In addition to the data revisions, I plan to test other models for classification such as XGBoost, Support Vector Machine .",
            "url": "https://dpendleton22.github.io/valuebyerror/logistic%20regression/nba/2021/02/14/mil_sac_data_logit_roast.html",
            "relUrl": "/logistic%20regression/nba/2021/02/14/mil_sac_data_logit_roast.html",
            "date": " • Feb 14, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Data Cleaning & Pipelines",
            "content": "The Jist: Data Cleaning is critical before developing a model . The data exploration post showed how to use knowledge about a dataset to interpret information. Since we know how the 2017-2019 seasons went for the Milwuakee Bucks and Sacramento Kings we can now plan out our machine learning problem. The machine learning model will attempt to predict the outcome of an NBA game before it actually occurs. We can start with using a logistic regression model to get a probabilistic output but we can look into other classification models after we give this one a go. This article outlines the most imperative portion of a machine learning project, outlining the problem and preparing the data. . Part 1: Data Exploration . This post is a continuation of the data exploration post where we explored the 2017-2019 seasons for the Milwuakee Bucks and the Sacramento Kings. Feel free to hop out and pop back in if you want to see the data described and explored: . Part 1 post: Data Exploration with NBA Data . . Set all the necessary paths for the data . The data was provided by https://www.basketball-reference.com/. They are a great source for anyone interested in sports analytics as an initial introduction. I can go into details later within the project to note the importance of detail in sports data. . Using the pathlib library from pandas it&#39;s straightforward getting all the data file names set. Setting a base folder name is a good method to simply call each dataset path by their name. Another method to get each dataset path would be to use the glob library to search the dataset folder for files with csv extensions . DATA_FOLDER = Path(os.getcwd(), &#39;mil_sac_data&#39;) sac_2017_szn = Path(DATA_FOLDER, &#39;sac_2017_2018_szn.csv&#39;) sac_2018_szn = Path(DATA_FOLDER, &#39;sac_2018_2019_szn.csv&#39;) mil_2017_szn = Path(DATA_FOLDER, &#39;mil_2017_2018_szn.csv&#39;) mil_2018_szn = Path(DATA_FOLDER, &#39;mil_2018_2019_szn.csv&#39;) . . Let&#39;s review one of the datasets to determine how they all need to be cleaned . sac_2017_df = pd.read_csv(sac_2017_szn, header=[0,1]) . Hold up, why are you setting the header argument? . Most times than not, calling pd.read_csv(&quot;filename&quot;) with no additional arguments would read in a dataframe as expected. In this instance, BasketballReference provides two headers in their csv so we need to let pandas know in order to process the dataset. Pandas read_csv() function has over 20 arguments that can be set depending on how the data is parsed and organized in the original file. So if your data is a little funky, the function may still be able to handle it. . Pandas read_csv() documentation: pandas.read_csv() . sac_2017_df.iloc[0:5, 0:15] . . Unnamed: 0_level_0 Unnamed: 1_level_0 Unnamed: 2_level_0 Unnamed: 3_level_0 Unnamed: 4_level_0 Unnamed: 5_level_0 Unnamed: 6_level_0 Unnamed: 7_level_0 Team . Rk G Date Unnamed: 3_level_1 Opp W/L Tm Opp FG FGA FG% 3P 3PA 3P% FT . 0 1 | 1 | 2017-10-18 | NaN | HOU | L | 100 | 105 | 42 | 88 | 0.477 | 8 | 23 | 0.348 | 8 | . 1 2 | 2 | 2017-10-20 | @ | DAL | W | 93 | 88 | 37 | 87 | 0.425 | 10 | 23 | 0.435 | 9 | . 2 3 | 3 | 2017-10-21 | @ | DEN | L | 79 | 96 | 31 | 85 | 0.365 | 8 | 22 | 0.364 | 9 | . 3 4 | 4 | 2017-10-23 | @ | PHO | L | 115 | 117 | 43 | 99 | 0.434 | 9 | 22 | 0.409 | 20 | . 4 5 | 5 | 2017-10-26 | NaN | NOP | L | 106 | 114 | 38 | 81 | 0.469 | 7 | 20 | 0.350 | 23 | . . Tip: Always view the dimensions of your data before analyzing it . print (f&quot;This dataset is {len(sac_2017_df)} in length and contains {len(sac_2017_df.columns)} columns&quot;) . This dataset is 82 in length and contains 41 columns . Using df.describe() is an easy and useful way to briefly view the distribution of the dataset across all the columns. This dataset is 82 rows in length which makes sense because there are 82 games in a regular season and contains 41 columns . sac_2017_df.iloc[0:5, 0:15].describe() . . Unnamed: 0_level_0 Unnamed: 1_level_0 Unnamed: 6_level_0 Unnamed: 7_level_0 Team . Rk G Tm Opp FG FGA FG% 3P 3PA 3P% FT . count 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | . mean 3.000000 | 3.000000 | 98.600000 | 104.000000 | 38.200000 | 88.000000 | 0.434000 | 8.400000 | 22.000000 | 0.381200 | 13.800000 | . std 1.581139 | 1.581139 | 13.612494 | 12.144958 | 4.764452 | 6.708204 | 0.044486 | 1.140175 | 1.224745 | 0.038855 | 7.120393 | . min 1.000000 | 1.000000 | 79.000000 | 88.000000 | 31.000000 | 81.000000 | 0.365000 | 7.000000 | 20.000000 | 0.348000 | 8.000000 | . 25% 2.000000 | 2.000000 | 93.000000 | 96.000000 | 37.000000 | 85.000000 | 0.425000 | 8.000000 | 22.000000 | 0.350000 | 9.000000 | . 50% 3.000000 | 3.000000 | 100.000000 | 105.000000 | 38.000000 | 87.000000 | 0.434000 | 8.000000 | 22.000000 | 0.364000 | 9.000000 | . 75% 4.000000 | 4.000000 | 106.000000 | 114.000000 | 42.000000 | 88.000000 | 0.469000 | 9.000000 | 23.000000 | 0.409000 | 20.000000 | . max 5.000000 | 5.000000 | 115.000000 | 117.000000 | 43.000000 | 99.000000 | 0.477000 | 10.000000 | 23.000000 | 0.435000 | 23.000000 | . Merge multi index headers and remove unwanted tags . In stead of indexing by columns with this notation, . sac_2017_df[(&#39;Unnamed: 0_level_0&#39;, &#39;Rk&#39;)] . we need to merge the header columns to allow for this type of indexing . sac_2017_df[&#39;Rk&#39;] . Lets do a quick magic wave of the hand and merge these headers together . Before: . sac_2017_df.columns[5:15] . MultiIndex([(&#39;Unnamed: 5_level_0&#39;, &#39;W/L&#39;), (&#39;Unnamed: 6_level_0&#39;, &#39;Tm&#39;), (&#39;Unnamed: 7_level_0&#39;, &#39;Opp&#39;), ( &#39;Team&#39;, &#39;FG&#39;), ( &#39;Team&#39;, &#39;FGA&#39;), ( &#39;Team&#39;, &#39;FG%&#39;), ( &#39;Team&#39;, &#39;3P&#39;), ( &#39;Team&#39;, &#39;3PA&#39;), ( &#39;Team&#39;, &#39;3P%&#39;), ( &#39;Team&#39;, &#39;FT&#39;)], ) . merged_columns = sac_2017_df.columns.map(&#39;.&#39;.join) . . After: . merged_columns[5:15] . . Index([&#39;Unnamed: 5_level_0.W/L&#39;, &#39;Unnamed: 6_level_0.Tm&#39;, &#39;Unnamed: 7_level_0.Opp&#39;, &#39;Team.FG&#39;, &#39;Team.FGA&#39;, &#39;Team.FG%&#39;, &#39;Team.3P&#39;, &#39;Team.3PA&#39;, &#39;Team.3P%&#39;, &#39;Team.FT&#39;], dtype=&#39;object&#39;) . . Note: Lets break that piece of code above down for a sec: sac_2017_df.columns.map(&#8217;.&#8217;.join) is calling the str.join() function where the str is &#8217;.&#8217; for each column with the .map() function . Now with the columns merged, we can keep the prefixed descriptions such as Team and Opponent so we know whose stats we&#39;re viewing but prefixes like &#39;Unnamed: 0_level_0&#39; are no use to us. . We can use regular expressions to remove the unneeded text in some of our column names . sac_2017_df.columns = merged_columns.str.replace(r&quot;Unnamed: [0-9]_level_[0-9].&quot;, &#39;&#39;, regex=True) . sac_2017_df.iloc[0:5, 0:15] . Rk G Date Unnamed: 3_level_1 Opp W/L Tm Opp Team.FG Team.FGA Team.FG% Team.3P Team.3PA Team.3P% Team.FT . 0 1 | 1 | 2017-10-18 | NaN | HOU | L | 100 | 105 | 42 | 88 | 0.477 | 8 | 23 | 0.348 | 8 | . 1 2 | 2 | 2017-10-20 | @ | DAL | W | 93 | 88 | 37 | 87 | 0.425 | 10 | 23 | 0.435 | 9 | . 2 3 | 3 | 2017-10-21 | @ | DEN | L | 79 | 96 | 31 | 85 | 0.365 | 8 | 22 | 0.364 | 9 | . 3 4 | 4 | 2017-10-23 | @ | PHO | L | 115 | 117 | 43 | 99 | 0.434 | 9 | 22 | 0.409 | 20 | . 4 5 | 5 | 2017-10-26 | NaN | NOP | L | 106 | 114 | 38 | 81 | 0.469 | 7 | 20 | 0.350 | 23 | . There is still an &#39;Unnmaed: 3_level_1&#39; tag after the regex processing which represents if the team of interest was playing home or away. We won&#39;t even be using this column as is so we can just process our new column and drop &#39;Unnamed: 3_level_1&#39; after. . The existing column consists of discreet values &#39;NaN&#39; or @ indication if the team was playing at home or away for this instance. We can simply check if the row value is NaN using the .isnull() function in pandas and set those values as a new column . sac_2017_df[&#39;playing_home&#39;] = sac_2017_df[&#39;Unnamed: 3_level_1&#39;].isnull() . Now that we have our column we can simply drop the existing &quot;Unnamed: 3_level_1&quot; column because &quot;playing_home&quot; represents the same thing now but with true and false values . sac_2017_df.drop(columns=[&#39;Unnamed: 3_level_1&#39;], inplace=True) . sac_2017_df.iloc[0:5, 0:15] . Rk G Date Unnamed: 3_level_1 Opp W/L Tm Opp Team.FG Team.FGA Team.FG% Team.3P Team.3PA Team.3P% Team.FT . 0 1 | 1 | 2017-10-18 | NaN | HOU | L | 100 | 105 | 42 | 88 | 0.477 | 8 | 23 | 0.348 | 8 | . 1 2 | 2 | 2017-10-20 | @ | DAL | W | 93 | 88 | 37 | 87 | 0.425 | 10 | 23 | 0.435 | 9 | . 2 3 | 3 | 2017-10-21 | @ | DEN | L | 79 | 96 | 31 | 85 | 0.365 | 8 | 22 | 0.364 | 9 | . 3 4 | 4 | 2017-10-23 | @ | PHO | L | 115 | 117 | 43 | 99 | 0.434 | 9 | 22 | 0.409 | 20 | . 4 5 | 5 | 2017-10-26 | NaN | NOP | L | 106 | 114 | 38 | 81 | 0.469 | 7 | 20 | 0.350 | 23 | . In order to prepare this data for a logistic regression model, we will also need to convert the non-numeric columns we plan to use to numerical values. Specifically converting the column of interest &quot;W/L&quot; to a numeric representation . sac_2017_df[&#39;dub&#39;] = sac_2017_df[&#39;W/L&#39;] == &#39;W&#39; . True values in this new column represent the team of interest got the dub or the Wu as Mastah Killah would say #WuTang #ATLUnited . sac_2017_df.iloc[0:5, -10:] . Opponent.FTA Opponent.FT% Opponent.ORB Opponent.TRB Opponent.AST Opponent.STL Opponent.BLK Opponent.TOV Opponent.PF dub . 0 29 | 0.931 | 12 | 44 | 19 | 7 | 3 | 14 | 14 | False | . 1 21 | 0.714 | 7 | 36 | 19 | 8 | 7 | 12 | 13 | True | . 2 20 | 0.600 | 18 | 58 | 25 | 7 | 2 | 16 | 19 | False | . 3 27 | 0.852 | 6 | 45 | 20 | 6 | 5 | 20 | 25 | False | . 4 23 | 0.739 | 10 | 47 | 22 | 5 | 3 | 15 | 24 | False | . Might as well make a pipeline . We have established, at least, our first pass at preparing the dataset. Since we will have to prepare the other dataframes in a similar way we can mitigate this by creating a data pipeline. This pipeline will take each original dataframe in and run the same preprocessing steps. This ensures everything is going through the same steps. Pipelines are not required but it will help you to stay organized . To make a pipeline we&#39;ll need to make the previous steps we created into a function to pass each dataframe through . def data_pipeline(df): test = df.columns.map(&#39;.&#39;.join).str.strip(&#39;.&#39;) df.columns = test.str.replace(r&quot;Unnamed: [0-9]_level_[0-9].&quot;, &#39;&#39;, regex=True) df[&#39;playing_home&#39;] = df[&#39;Unnamed: 3_level_1&#39;].isnull() df.drop(columns=[&#39;Unnamed: 3_level_1&#39;], inplace=True) df[&#39;dub&#39;] = df[&#39;W/L&#39;] == &#39;W&#39; df.drop(columns=[&#39;W/L&#39;], inplace=True) return df . Running the pipeline . We can consolidate the number of duplicate lines to run into a function and process all similar datasets with the same function. The code below reads in each dataset and immediately uses the pandas .pipe() function passing in the preprocessing function. Though we didn&#39;t use it, the pandas.DataFrame.pipe() function allows positional and keyword arguments to be passed in with the function to run. . sac_2017_df = pd.read_csv(sac_2017_szn, header=[0, 1]).pipe(data_pipeline) sac_2018_df = pd.read_csv(sac_2018_szn, header=[0, 1]).pipe(data_pipeline) mil_2017_df = pd.read_csv(mil_2017_szn, header=[0, 1]).pipe(data_pipeline) mil_2018_df = pd.read_csv(mil_2018_szn, header=[0, 1]).pipe(data_pipeline) . The Jist . Data cleaning and preprocessing is not the most fun job in data science but it sets the foundation for whatever model that will be used. There are a number of automated tools and software that claim to automatically clean datasets but from this notebook it&#39;s easy to see it isn&#39;t a cookie cutter process. This dataset was cleaned knowing a logistic regression model was going to be used and the tags to fit the model but various other methods could have been used to clean this data. Such as scaling the dataset or one-hot encoding all string columns (but I plan to not use most of them so I didn&#39;t bother). An experienced engineer once told me coding should be the easy part. That statement didn&#39;t hit me at first but I now understand that statement speaks to the importance of understanding what you want to do with the data or a model. Speaking of model, in the next post we&#39;ll get right into logistic regression models and how to measure their success cause .",
            "url": "https://dpendleton22.github.io/valuebyerror/data%20cleaning/data%20preparation/pandas.pipe/nba/2021/01/18/mil_sac_data_prep.html",
            "relUrl": "/data%20cleaning/data%20preparation/pandas.pipe/nba/2021/01/18/mil_sac_data_prep.html",
            "date": " • Jan 18, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Exploratory Data Analysis",
            "content": "Exploratory Data Analysis . Originally created: Feb 12, 2019 . TL;DR . Exploratory analysis is an underrated process that allows us to gain some critical knowledge about the data to discover trends and outliers. You need to explore your data before you try and predict your data . What is this article and what will you show me? . For quite some time I’ve been meaning to jump into the realm of sports analytics but there were way too many things to take into account and five-thirty-eight is killing’ the game so why even bother? Because it’s real out here and I still have questions that are not answered, and I ain&#39;t going to Sway for the answers. . . So who better than myself. My plan is to provide analytics for teams that interest me. There’s no point in choosing my LeBron-less Cavs since we all know what fate lies ahead for them but as Joel Embiid said we’re still the defending eastern conference champs! No, instead I chose two teams that aren’t favorites to win the finals but provide enough interest to study their playing style. Those teams are the Milwaukee Bucks and the Sacramento Kings…… . ”Imma let you finish David, you had me with the Bucks but the Kings my dude….really?” . -Yes, really, the Sacramento Kings are a team to watch for. . There’s really no need to explain the Bucks as they have the Greek Freak, Giannis Antetokounmpo. This kid, yes I said kid (he was born in 1994!) is 6’11 and 245 lbs (sweet Jesus this kid is a freak). His time with he Bucks was eventually going to come and with LeBron leaving that time might be now. The Milwaukee Bucks (MIL) and Sacramento Kings (SAC) were initially selected from my knowledge of watching basketball over the years with no analytic knowledge to backup my initial selections. In an effort to support these claims, let’s do some exploratory analysis to backup my initial selections . In this article I will show: . Two exploratory analysis plots, why and when to use them | Why my selections for SAC and MIL are valid numerically | The conducive information that can be analyzed from exploratory data | . Lets jump right into an imperative figure for summary detail, the box plot. . The box plot . What’s the jist? . Observing the overall percentage distribution of the dataset will give useful insight of where the subject of interest lies especially when there is a benchmark | . Some people think you need have have your programming skills at a Bruce Lee level to do analytics or machine learning, I believe a primary skill needed is an imagination and ability to ask the right questions . Why is that? I’ll write a 2-piece on this later but how else do you know what you’re going to code analytically if you don’t know what information you want? Just follow along for a sec and you’ll see where I’m getting at. Before I get into analyzing MIL or SAC I need to know where they sit in the league. This article will just observe the general stats (assists, turnovers, field goal percentage, etc) before getting into the weeds with the advanced analytics that are becoming standard in the league. To start off let’s do a little underhand pitch on this: . How well did MIL and SAC average amongst the entire league last year, 2017-2018? . Before we start getting jiggy with it, what are some things we already know about what happened last year (2017-2018)? GSW shot the lights out! This KD added team terrorized the entire NBA and made a snooze fest out of the regular and post season, even J.R. Smith went to sleep during the finals . . So to answer this question we need the right type of plot to give us a quick glance of the entire NBA league over the season. This calls for a box plot summary of the stats we wish to view. A quick summary for those who dosed off during stats about box plots and when they’re useful. A box plot provides a graphical percentage summary of the distribution of the data. With this information in mind, I’m gonna guess GSW are at the top of a shooting percentage (FG and/or 3P). Knowing where the 2-time NBA champs sat gives us a benchmark to compare how SAC and MIL are with those same stats. . Basketball Metrics . Hold up, wait a minute… . Before we jump stop into this box plot, lets take a quick euro step to learn about the various measured stats in this article. . . Shoutout to the WNBA for this one #WatchMeWork . NBA stats have two distinct categories, basic and advanced. All the stats used in this article are categorized as basic stats; which are stats that can be determined in-game. Such as keeping track of a teams points or how many blocks they have. Advanced stats is a newer trend in the field and is where things get a little, well…advanced. They can include stats such as player ratings or player efficiency. These advanced stats are developed calculations that are derived by using the basic stats in the algorithms. I’ll dive into the different advanced stats in another article but for the basic stats you’ll see the following: . FG% - Field goal percentage | 3P% - Three point percentage | FT% - Free throw percentage | AST - Total assists | BLK - Total blocks | ORB - Total offensive rebounds | TRB - Total rebounds, offensive and defensive | TOV - Total turnovers | . All basic stats in this article are on a team level but they can also be used to measure an individual player’s performance. . NBA 2017 - 2018 League Summary . . What is the take away from observing the 2017-2018 season overall? . Though 40% does not initially sound great, having ~35-37% 3P% shooting is great for a team and SAC was pretty efficient from the 3 point line last year | Teams usually average between 40 - 50 rebounds for a game with most of them being defensive rebounds | There is a wide range for the FT% but teams still averaged more than 70% for the season | MIL had the lowest total rebounds in the league | SAC was surprisingly efficient from the 3 point line Now that we got a glance at last season, let’s see how SAC and MIL are doing this current NBA season: | . . What is the current 2018-2019 season takeaway? . SAC is still efficient from the 3 point line and increased their field goal percentage from the court | MIL went from the worst total rebounding team to the league’s best for the season | MIL and SAC are moving the ball more efficiently with significant increases in AST | . The box plot is great as a quick glance to see the trends within the data but there’s another plot that will slice the data into an even finer exploration for analysis . A histogram will show how frequently a specific stat occurred, and with a little extra sauce we can split that data to view home and away wins and losses. . . Shoutout to Giannis with the extra sauce for this post . The Histogram . What’s the jist? . Using a histogram with a little bit of sauce, we’ll be able to see how well a team performed home vs away and if they more frequently won or lost those games | . How well did MIL and SAC play during home and away games? . How to read the facet grid histogram: . The facet grid allows for the data to be sectioned or categorized based on specific categorical values. In this case we sectioned the data off for home and away games (varied by column) In additon I wanted to categorigze games that were won and lost (varied by rows). . So in this article for all the listed facet grid histograms the formatting is as follows: . top left - home games that were won | bottom left - away games that were won | top right, home games that were lost | bottom right - away games that were lost | . Milwuakee Bucks and Rebounding . Of the 32 games the Milwaukee Bucks have managed 42 or more total rebounds they’ve won 75% of those games or 26 games to be exact. The league’s average for team’s total rebounds is 43! This means it is more than probable for this team to increase their team rebounding Note the league FGA, FG and Milwaukee’s FGA, FG (MIL shoots 3 shots less than the league FGA but maintains to be within the league mean of FG with 39) . . MIL is currently second in the eastern conference with a 41-13 record. Last season, though MIL was able to produce decent stats they were the weakest in the league for total rebounds. It was noted when MIL was able to rebound they won 75% of those games. This season with MIL being second in the east and a likely eastern conference contender, they are the top total rebound team in the league. Of the 45 times MIL has been able to produce 43 or more total rebounds they’ve won 34 of those games or 79%. . . Sacramento Kings and Ball Movement . Of the 47 times the Sacramento Kings have been able to produce 23 or more assists they’ve won 74% of those game (35 games to be exact) Though assists may not seem imperative to a team’s production, the defending champions Golden State Warriors led the league in assists with 29.3 which were one of the three outliers from the box plot. The other two outliers were the New Orleans Pelicans and the Philadelphia 76ers (both 2017-2018 playoff teams) . . When observing MIL assists, they only have a 60% increase in wins when producing over 22 assists (28 wins out of 46 games) Last season SAC won 74% of their games (24/34) when they had 23 or more assists (which was their mean assists last year). This year through 55 games that mean has increased to 25 assists over the current season. Of the 34 games SAC has been able to produce 25 or more assists, they’ve won 20 of those games or 58%. The assist increase is directly correlated to their FG% increase from 45 - 47% as an assist is only completed after a made FG. Of the 23 times SAC shot 45 or more percent on the court they’ve won 14 of those games or 60%. . . Sacramento is currently 9th in the Western conference with a 29-23 record and they have two 2019 all-stars in De’Aron Fox and Buddy Hield. They’re a team to lookout for in the next few seasons and even for the second half of this year. . The Jist . Exploring and understanding these basic stats (BLK, AST, etc.) and how they are utilized with each team gives us a better understanding of what stats are important and to which teams. A box plot is useful to determine how well a team is competing compared to the league but in order to get a detail of overall performance a histogram is better. A team such as Sacramento, with a young future star point guard, they are expected to do better with ball movement and setting up efficient shots. Milwaukee having a number of tall and lanky players such as the Greek Freak, Thon Maker and Brook Lopez, they’re expected to not give second shot opportunities to their opponents by rebounding better. With this information we are able to select what tags are imperative in creating a predictive model for the second half of the season. .",
            "url": "https://dpendleton22.github.io/valuebyerror/data%20exploration/box%20plots/histograms/nba/2021/01/14/nba-analysis-post.html",
            "relUrl": "/data%20exploration/box%20plots/histograms/nba/2021/01/14/nba-analysis-post.html",
            "date": " • Jan 14, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I remember a joke during my undergrate years, “if engineers got everything right the first time they’d be called magicians”. As a foundational engineer I am skilled at correcting myself and learning from my mistakes. Data science is far from perfect but there are plenty of skills I gained as an engineer that I use in my data science job. One major skill is not over reacting from errors and learning how to measure them and not repeat them. ValuebyError is my open notes on the steps I have and continue to take as I gain my skills in the data science field. Posts will include projects I’ve done, lessons learned from projects and general thoughts on things occuring in the data science field. . I spent about 3 years as an engineer with some software experience and made the transition to being a dedicated data scientist. I have 2 years of applied data scinece research experience and a masters in computer science from Georgia Tech. .",
          "url": "https://dpendleton22.github.io/valuebyerror/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://dpendleton22.github.io/valuebyerror/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}