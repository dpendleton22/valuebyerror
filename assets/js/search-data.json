{
  
    
        "post0": {
            "title": "The McFly Effect: Logitic Regression",
            "content": "The McFly Effect . When someone gets into data science there are usually two things they&#39;d like to predict. Sports game outcomes and the stock market...and I&#39;m no different. One of the first projects I did in data science (2 years ago) was predicting the outcome of NBA games . . Using a pipeline . We can use the pipeline created in the last post to preprocess our data. Using pipelines is an efficient way to ensure there is no data leakage when developing a model. Data leakage is when data that normally would not be accessible to the model during usage is present in the training set. This can be the column or interest or an extra column that is not always available. . . def data_pipeline(df): test = df.columns.map(&#39;.&#39;.join).str.strip(&#39;.&#39;) df.columns = test.str.replace(r&quot;Unnamed: [0-9]_level_[0-9].&quot;, &#39;&#39;, regex=True) df[&#39;playing_home&#39;] = df[&#39;Unnamed: 3_level_1&#39;].isnull() df.drop(columns=[&#39;Unnamed: 3_level_1&#39;, &#39;Unnamed: 24_level_0.Unnamed: 24_level_1&#39;], inplace=True) df[&#39;dub&#39;] = df[&#39;W/L&#39;] == &#39;W&#39; df.drop(columns=[&#39;W/L&#39;], inplace=True) return df . sac_2017_df = pd.read_csv(sac_2017_szn, header=[0, 1]).pipe(data_pipeline) sac_2018_df = pd.read_csv(sac_2018_szn, header=[0, 1]).pipe(data_pipeline) mil_2017_df = pd.read_csv(mil_2017_szn, header=[0, 1]).pipe(data_pipeline) mil_2018_df = pd.read_csv(mil_2018_szn, header=[0, 1]).pipe(data_pipeline) . Why are we looking at this NBA data from 2017 my guy? . Good question, which I have a few answers to. First, this is the earliest data science project I can remember doing outside of my master&#39;s class. In addition, when working with time series data it can be beneifical to work out your model with historic data. If the model doesn&#39;t work well on previous events we can be pretty sure it will have the same performance on more up to date data. Lastly, I presented it during my job interview which I currently have today. . For this specific post we will only work with the Sacramento Kings&#39; schedule . sac_szn = pd.concat([sac_2017_df, sac_2018_df]) . The main objective is to determine if a team is going to win the game of interest so let&#39;s remove that from the dataset and make it its own labeled series. This is apart of that data leakage conflict I wrote about earlier. We want to avoid the model from seeing the actual game outcomes . In addition to the win/loss column which I named &quot;dub&quot;. I dropped the &#39;playing home&#39; column which teams me if the Kings are playing home or away that game. This is a boolean indicator which is predetrmined before the game so instead of avergaing this value, I can use it&#39;s original value to infer if the Kings have home court advantage. . sac_szn_home = sac_szn[&#39;playing_home&#39;] . sac_szn_dub = sac_szn[&#39;dub&#39;] . sac_szn.drop(columns=[&#39;Rk&#39;, &#39;G&#39;, &#39;Date&#39;, &#39;Opp&#39;, &#39;Tm&#39;, &#39;Opp&#39;, &#39;dub&#39;, &#39;playing_home&#39;], inplace=True) . In order to predict the outcome of a game we of course can&#39;t look at the actual stats for that game and use them as an input. We are trying to predict the outcome before it happens so that includes the stats for the game. Instead we need to use some prior knowledge from previous games to make our prediction. I remember deciding to use a 3 day roliing average from both stat lines. This makes a lot of assumptions which I&#39;ll explain later in this post . Since the dataset is already a pandas dataframe we can just create the rolling average using the &quot;.rolling()&quot; function. This takes the length of time to roll as input which is set to 3. This is because each row represents a game. The &quot;.mean()&quot; function takes the average across the 3 games as the new value for the rolling results. This can also be used to create a rolling sum, rolling difference, etc. . sac_szn_rolling = sac_szn.rolling(3).mean() . sac_szn_rolling[&#39;playing_home&#39;] = sac_szn_home . sac_szn_rolling[&#39;dub&#39;] = sac_szn_dub . After taking the rolling average, the game outcome needs to be added back in and the indicator of playing at home or away. In addition the outcome of the game needs to be shifted. This shift removes the stats for the game of interest from being used as an input for the prediction. . sac_szn_rolling[[&#39;playing_home&#39;, &#39;dub&#39;]] = sac_szn_rolling[[&#39;playing_home&#39;, &#39;dub&#39;]].shift(periods=-1) . sac_szn_rolling[[&#39;playing_home&#39;, &#39;dub&#39;]] = sac_szn_rolling[[&#39;playing_home&#39;, &#39;dub&#39;]].astype(float) . We can remove the edges of the dataframe as these rows do not have information from the rolling average and game outcome shift . sac_szn_rolling = sac_szn_rolling.iloc[2:-1, :] . Train / Validation Split . I&#39;ll get more into details in more posts about best practices but we split this data into two segments, training and validation splits. The training set is used to develop the model, the model makes its adjustments with this dataset. The validation set is solely used to check the outcome of training the model. This portion of the data is never used to train the model. . For this specific dataset because it is a schedule of games, I decided to train on the first 80% of games and validate on the later 20%. In other cases, the data can be shuffled then split but that can be explored later . split = int(len(sac_szn_rolling)*0.8) . sac_szn_train, sac_szn_train_label = sac_szn_rolling.iloc[:split, :], sac_szn_rolling[&#39;dub&#39;][:split] . sac_szn_test, sac_szn_test_label = sac_szn_rolling.iloc[split:, :], sac_szn_rolling[&#39;dub&#39;][split:] . %%capture sac_szn_train.drop(columns=[&#39;dub&#39;], inplace=True) sac_szn_test.drop(columns=[&#39;dub&#39;], inplace=True) . When working with data preprocess and data splitting, it&#39;s a good idea to check that no rows of information are lost during the process. In this case, the original number of rows should equal the sum of the training and test split. . len(sac_szn_rolling), len(sac_szn_train) + len(sac_szn_test) . (161, 161) . Logistic Regression and How it works . What is Logistic Regression or Logit . Logistic regression is a regression model commonly used for classification which calculates the probability of an outcome using parameter weights on the input data. This calculated probability is used against an activation function, sigmoid, to give a predicted classification output | . How is the algorithm used?? . This algorithm is commonly used for classification of discreet values. It can be used for multivariate classification but there is risk the parameter weights that are calculated will be unstable if each of the classification types are significantly different in distributions | . Ok, so how does it actually work? . The equation logistic regression uses to calculate the probability of the input values being 0 or 1 is the following: . $P(X) = frac{e^{ beta_0 + beta_p * X}}{1 + e^{ beta_0 + beta_p * X}}$ . where the beta variables are the weights cofficients which are multiplied with the input values, X. The number of beta values is equal to the number of input X values. . For binary classification - Parameter weights which match the length of independent variables are adjusted using a loss function and gradient descent. The probability of the input variables with the parameter weights equaling the value of 1 is calculated with the parameter weights. That probability is passed through the sigmoid activation function to get a binary classification prediction. Then the loss is calculated using gradient descent. . | Logistic regression could use least squares ,like linear regression, to determine the parameters but instead maximum likelihood is used. Maximum likelihood attempts to find values for the parameters in a way that the probability of all individuals that fit a specific classification would be 1 and 0 for the others that are not in that class. . | . Do input data need to be scaled? . No the values in the model do not need to be scaled | As long as the data used to make a prediction is on the same scale as the data used to optimize the parameter values | . We can use a logistic regression model library to train and test the model. The first library we can use is the logistic regression model from sklearn . clf = LogisticRegressionCV(solver=&#39;liblinear&#39;, max_iter=200).fit(sac_szn_train, sac_szn_train_label) . print (f&quot;The current number of features are: {len(clf.coef_[0])}&quot;) . The current number of features are: 33 . Class blanace . When working with a classification model, be sure to check the classification representation from each label. Imbalance within a dataset can affect the training of a model. The current datatset we have is imbalanced towards 0 or losses for the Kings. We can address this later if insatisfactory model results occur. . . for i in sac_szn_train_label.unique(): print (f&#39;Count for {i}: {len(sac_szn_train_label.loc[sac_szn_train_label == i])}&#39;) . Count for 0.0: 77 Count for 1.0: 51 . Initial model results: 60% looks better than a coin flip . clf.score(sac_szn_train, sac_szn_train_label) . 0.6015625 . The learning curve is your friend . The learning curve gives detail on the progress made during model training. It can be used to determine underfits and overfits of your model . %%capture train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(clf, sac_szn_train, sac_szn_train_label, train_sizes=np.linspace(0.5, 1.0, 10), return_times=True, verbose=10); . plt.title(&quot;Logistic Regression: Learning Curve&quot;) plt.plot(train_sizes, train_scores.mean(axis=1), label=&#39;training&#39;) plt.plot(train_sizes, test_scores.mean(axis=1), label=&#39;validation&#39;) plt.ylabel(&quot;Accuracy (%)&quot;) plt.xlabel(&quot;Epochs&quot;) plt.grid() plt.legend() . &lt;matplotlib.legend.Legend at 0x7fa39f4cda90&gt; . The overall shape of the curve makes sense but there appears to be some noise or something interesting with the data from the jump in accuracy. We can have a better look at how the algorithm is predicting with a confusion matrix . Confusion Matrix . The confusion matrix is a great visual tool for observing classification performance of a model. It includes a matrix showing the true positves, true negatives, false positives and false negatives of the algorithm on the data . Enter the Matrix: . cm = confusion_matrix(sac_szn_train_label, clf.predict(sac_szn_train)) fig, ax = plt.subplots(figsize=(8, 8)) ax.imshow(cm) ax.grid(False) ax.xaxis.set(ticks=(0, 1), ticklabels=(&#39;Predicted 0s&#39;, &#39;Predicted 1s&#39;)) ax.yaxis.set(ticks=(0, 1), ticklabels=(&#39;Actual 0s&#39;, &#39;Actual 1s&#39;)) ax.set_ylim(1.5, -0.5) for i in range(2): for j in range(2): ax.text(j, i, cm[i, j], ha=&#39;center&#39;, va=&#39;center&#39;, color=&#39;red&#39;) plt.show() . 60% doesn&#39;t look so good now... . This confusion matrx shows our model can easily get to 60% accuracy by simply predicting 0 or &#39;loss&#39; for the team which is not how we want our model to predict. The goal of the model is to generalize the data . . Generalizing the logistic regression algorithm . From the confusion matrix we can see the algorithm is simply preciting 0 (loss) everytime to get a 60% accuracy score. We can tune the logistic regression to regualarize the algorithm to make better predictions . clf = LogisticRegressionCV(solver=&#39;liblinear&#39;, Cs=1000, max_iter=200).fit(sac_szn_train, sac_szn_train_label) . clf.score(sac_szn_train, sac_szn_train_label) . 0.6015625 . %%capture train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(clf, sac_szn_train, sac_szn_train_label, train_sizes=np.linspace(0.5, 1.0, 10), return_times=True, verbose=10) . plt.title(&quot;Logistic Regression: Learning Curve&quot;) plt.plot(train_sizes, train_scores.mean(axis=1), label=&quot;training&quot;) plt.plot(train_sizes, test_scores.mean(axis=1), label=&#39;validation&#39;) plt.ylabel(&quot;Accuracy (%)&quot;) plt.xlabel(&quot;Epochs&quot;) plt.grid() plt.legend() . &lt;matplotlib.legend.Legend at 0x7fa39f073810&gt; . cm = confusion_matrix(sac_szn_train_label, clf.predict(sac_szn_train)) fig, ax = plt.subplots(figsize=(8, 8)) ax.imshow(cm) ax.grid(False) ax.xaxis.set(ticks=(0, 1), ticklabels=(&#39;Predicted 0s&#39;, &#39;Predicted 1s&#39;)) ax.yaxis.set(ticks=(0, 1), ticklabels=(&#39;Actual 0s&#39;, &#39;Actual 1s&#39;)) ax.set_ylim(1.5, -0.5) for i in range(2): for j in range(2): ax.text(j, i, cm[i, j], ha=&#39;center&#39;, va=&#39;center&#39;, color=&#39;red&#39;) plt.show() . Attempts to regularize the logistics regression in sklearn are showing no significant difference from the first results. Time to introduce StatsModels a great stastical library to use . Stats models logistic regression library . log_reg = sm.Logit(sac_szn_train_label, sac_szn_train).fit() . Optimization terminated successfully. Current function value: 0.551797 Iterations 6 . print (log_reg.summary()) . Logit Regression Results ============================================================================== Dep. Variable: dub No. Observations: 128 Model: Logit Df Residuals: 95 Method: MLE Df Model: 32 Date: Sat, 23 Jan 2021 Pseudo R-squ.: 0.1793 Time: 00:02:35 Log-Likelihood: -70.630 converged: True LL-Null: -86.064 Covariance Type: nonrobust LLR p-value: 0.5237 ================================================================================ coef std err z P&gt;|z| [0.025 0.975] -- Team.FG -0.4993 0.995 -0.502 0.616 -2.449 1.450 Team.FGA -0.0249 0.447 -0.056 0.956 -0.901 0.851 Team.FG% 10.2459 88.084 0.116 0.907 -162.396 182.887 Team.3P -0.0556 1.014 -0.055 0.956 -2.044 1.933 Team.3PA 0.0612 0.381 0.161 0.872 -0.686 0.808 Team.3P% -1.0472 23.788 -0.044 0.965 -47.671 45.576 Team.FT -0.0420 0.892 -0.047 0.962 -1.790 1.706 Team.FTA -0.1774 0.641 -0.277 0.782 -1.433 1.078 Team.FT% -3.9722 13.852 -0.287 0.774 -31.121 23.177 Team.ORB 0.1043 0.365 0.286 0.775 -0.610 0.819 Team.TRB -0.0082 0.233 -0.035 0.972 -0.464 0.448 Team.AST 0.0171 0.153 0.112 0.911 -0.282 0.316 Team.STL -0.0892 0.233 -0.382 0.702 -0.547 0.368 Team.BLK 0.4874 0.283 1.724 0.085 -0.067 1.041 Team.TOV -0.0883 0.326 -0.271 0.787 -0.728 0.551 Team.PF -0.1124 0.195 -0.576 0.564 -0.495 0.270 Opponent.FG 0.7371 0.928 0.794 0.427 -1.082 2.556 Opponent.FGA 0.1074 0.464 0.232 0.817 -0.801 1.016 Opponent.FG% -46.9855 76.885 -0.611 0.541 -197.678 103.707 Opponent.3P 0.8309 1.073 0.774 0.439 -1.272 2.934 Opponent.3PA -0.2885 0.420 -0.687 0.492 -1.111 0.534 Opponent.3P% -12.1495 33.259 -0.365 0.715 -77.337 53.038 Opponent.FT -0.5887 0.718 -0.820 0.412 -1.996 0.819 Opponent.FTA 0.7238 0.564 1.283 0.200 -0.382 1.830 Opponent.FT% 19.3377 16.380 1.181 0.238 -12.766 51.442 Opponent.ORB -0.1427 0.361 -0.396 0.692 -0.850 0.564 Opponent.TRB -0.3266 0.254 -1.286 0.199 -0.824 0.171 Opponent.AST -0.0918 0.149 -0.618 0.537 -0.383 0.200 Opponent.STL 0.0580 0.325 0.178 0.858 -0.579 0.695 Opponent.BLK 0.1431 0.212 0.674 0.500 -0.273 0.559 Opponent.TOV 0.3809 0.237 1.609 0.108 -0.083 0.845 Opponent.PF 0.1081 0.222 0.487 0.626 -0.327 0.543 playing_home 0.8333 0.511 1.632 0.103 -0.168 1.834 ================================================================================ . The current coefficient values in the output value most Team.FG%, Team.FT%, Opp.FG%, Opp.3P% and Opp.FT% . This is based on coeffient values and their contribution to the calculated probability. Observing the z-stastistical values none of them are large and none of the p values less than 0.01. This means we can not conclude there is a clear significance between these values and the team winning. Results such as these give me more insights to adding more data or revisitng the preprocessing method. . pred = log_reg.predict(sac_szn_train) . accuracy_score(sac_szn_train_label, list(map(round, pred))) . 0.734375 . This confusion matrix looks a lot more dispersed on predictions which is the behavior we want to see in the model . cm = confusion_matrix(sac_szn_train_label, list(map(round, pred))) fig, ax = plt.subplots(figsize=(8, 8)) ax.imshow(cm) ax.grid(False) ax.xaxis.set(ticks=(0, 1), ticklabels=(&#39;Predicted 0s&#39;, &#39;Predicted 1s&#39;)) ax.yaxis.set(ticks=(0, 1), ticklabels=(&#39;Actual 0s&#39;, &#39;Actual 1s&#39;)) ax.set_ylim(1.5, -0.5) for i in range(2): for j in range(2): ax.text(j, i, cm[i, j], ha=&#39;center&#39;, va=&#39;center&#39;, color=&#39;red&#39;) plt.show() . sac_szn_train_down = sac_szn_train[[&#39;playing_home&#39;, &#39;Opponent.TOV&#39;, &#39;Opponent.FT%&#39;, &#39;Opponent.FTA&#39;, &#39;Team.BLK&#39;, &#39;Opponent.TRB&#39;]] . log_reg_second = sm.Logit(sac_szn_train_label, sac_szn_train_down).fit() . Optimization terminated successfully. Current function value: 0.657341 Iterations 5 . print (log_reg_second.summary()) . Logit Regression Results ============================================================================== Dep. Variable: dub No. Observations: 128 Model: Logit Df Residuals: 122 Method: MLE Df Model: 5 Date: Thu, 21 Jan 2021 Pseudo R-squ.: 0.02236 Time: 21:35:28 Log-Likelihood: -84.140 converged: True LL-Null: -86.064 Covariance Type: nonrobust LLR p-value: 0.5715 ================================================================================ coef std err z P&gt;|z| [0.025 0.975] -- playing_home 0.3219 0.370 0.870 0.384 -0.403 1.047 Opponent.TOV 0.0465 0.076 0.611 0.541 -0.103 0.196 Opponent.FT% 0.2939 2.150 0.137 0.891 -3.921 4.508 Opponent.FTA -0.0244 0.048 -0.505 0.614 -0.119 0.070 Team.BLK 0.2379 0.162 1.468 0.142 -0.080 0.556 Opponent.TRB -0.0428 0.044 -0.971 0.332 -0.129 0.044 ================================================================================ . pred = log_reg_second.predict(sac_szn_test[[&#39;playing_home&#39;, &#39;Opponent.TOV&#39;, &#39;Opponent.FT%&#39;, &#39;Opponent.FTA&#39;, &#39;Team.BLK&#39;, &#39;Opponent.TRB&#39;]]) . accuracy_score(sac_szn_test_label, list(map(round, pred))) . 0.5454545454545454 . cm = confusion_matrix(sac_szn_test_label, list(map(round, pred))) fig, ax = plt.subplots(figsize=(8, 8)) ax.imshow(cm) ax.grid(False) ax.xaxis.set(ticks=(0, 1), ticklabels=(&#39;Predicted 0s&#39;, &#39;Predicted 1s&#39;)) ax.yaxis.set(ticks=(0, 1), ticklabels=(&#39;Actual 0s&#39;, &#39;Actual 1s&#39;)) ax.set_ylim(1.5, -0.5) for i in range(2): for j in range(2): ax.text(j, i, cm[i, j], ha=&#39;center&#39;, va=&#39;center&#39;, color=&#39;red&#39;) plt.show() . Next Steps: Lets LEVEL UP! . As within any data science project, there are a number of next steps or changes that can be added to a project to level up. For this specific project I believe adding more descriptive information about the data and each team would assist the logisitc regression model. To be more specific, there are advanced analytics used in the world of sports analytics that would help this project. For example, there is an ELO rating . .",
            "url": "https://dpendleton22.github.io/valuebyerror/logistic%20regression/nba/2021/02/14/mil_sac_data_logit_roast.html",
            "relUrl": "/logistic%20regression/nba/2021/02/14/mil_sac_data_logit_roast.html",
            "date": " • Feb 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Data Cleaning & Pipelines",
            "content": "The Jist: Data Cleaning is critical before developing a model . The data exploration post showed how to use knowledge about a dataset to interpret information. Since we know how the 2017-2019 seasons went for the Milwuakee Bucks and Sacramento Kings we can now plan out our machine learning problem. The machine learning model will attempt to predict the outcome of an NBA game before it actually occurs. We can start with using a logistic regression model to get a probabalistic output but we can look into other classification models after we give this one a go. This article outlines the most imperative portion of a machine learning project, outlining the problem and preparing the data. . Part 1: Data Exploration . This post is a continuation of the data exploration post where we explored the 2017-2019 seasons for the Milwuakee Bucks and the Sacramento Kings. Feel free to hop out and pop back in if you want to see the data described and explored: . Part 1 post: Data Exporlation with NBA Data . . Set all the necessary paths for the data . The data was provided by https://www.basketball-reference.com/. They are a great source for anyone interested in sports analytics as an intial introduction. I can go into details later within the project to note the importance of detail in sports data. . Using the pathlib library from pandas it&#39;s straightforward getting all the data file names set. Setting a base folder name is a good method to simply call each dataset path by their name. Another method to get each dataset path would be to use the glob library to search the dataset folder for files with csv extensions . DATA_FOLDER = Path(os.getcwd(), &#39;mil_sac_data&#39;) sac_2017_szn = Path(DATA_FOLDER, &#39;sac_2017_2018_szn.csv&#39;) sac_2018_szn = Path(DATA_FOLDER, &#39;sac_2018_2019_szn.csv&#39;) mil_2017_szn = Path(DATA_FOLDER, &#39;mil_2017_2018_szn.csv&#39;) mil_2018_szn = Path(DATA_FOLDER, &#39;mil_2018_2019_szn.csv&#39;) . . Let&#39;s review one of the datasets to determine how they all need to be cleaned . sac_2017_df = pd.read_csv(sac_2017_szn, header=[0,1]) . Hold up, why are you setting the header argument? . Most times than not, calling pd.read_csv(&quot;filename&quot;) with no additional arguments would read in a dataframe as expected. In this instance, BasketballReference provides two headers in their csv so we need to let pandas know in order to process the dataset. Pandas read_csv() function has over 20 arguments that can be set depending on how the data is parsed and organized in the original file. So if your data is a little funky, the function may still be able to handle it. . Pandas read_csv() documentation: pandas.read_csv() . sac_2017_df.iloc[0:5, 0:15] . . Unnamed: 0_level_0 Unnamed: 1_level_0 Unnamed: 2_level_0 Unnamed: 3_level_0 Unnamed: 4_level_0 Unnamed: 5_level_0 Unnamed: 6_level_0 Unnamed: 7_level_0 Team . Rk G Date Unnamed: 3_level_1 Opp W/L Tm Opp FG FGA FG% 3P 3PA 3P% FT . 0 1 | 1 | 2017-10-18 | NaN | HOU | L | 100 | 105 | 42 | 88 | 0.477 | 8 | 23 | 0.348 | 8 | . 1 2 | 2 | 2017-10-20 | @ | DAL | W | 93 | 88 | 37 | 87 | 0.425 | 10 | 23 | 0.435 | 9 | . 2 3 | 3 | 2017-10-21 | @ | DEN | L | 79 | 96 | 31 | 85 | 0.365 | 8 | 22 | 0.364 | 9 | . 3 4 | 4 | 2017-10-23 | @ | PHO | L | 115 | 117 | 43 | 99 | 0.434 | 9 | 22 | 0.409 | 20 | . 4 5 | 5 | 2017-10-26 | NaN | NOP | L | 106 | 114 | 38 | 81 | 0.469 | 7 | 20 | 0.350 | 23 | . . Tip: Always view the dimensions of your data before analyzing it . print (f&quot;This dataset is {len(sac_2017_df)} in length and contains {len(sac_2017_df.columns)} columns&quot;) . This dataset is 82 in length and contains 41 columns . Using df.describe() is an easy and useful way to breifly view the distribution of the dataset across all the columns. This dataset is 82 rows in length which makes sense because there are 82 games in a regular season and contains 41 columns . sac_2017_df.iloc[0:5, 0:15].describe() . . Unnamed: 0_level_0 Unnamed: 1_level_0 Unnamed: 6_level_0 Unnamed: 7_level_0 Team . Rk G Tm Opp FG FGA FG% 3P 3PA 3P% FT . count 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | . mean 3.000000 | 3.000000 | 98.600000 | 104.000000 | 38.200000 | 88.000000 | 0.434000 | 8.400000 | 22.000000 | 0.381200 | 13.800000 | . std 1.581139 | 1.581139 | 13.612494 | 12.144958 | 4.764452 | 6.708204 | 0.044486 | 1.140175 | 1.224745 | 0.038855 | 7.120393 | . min 1.000000 | 1.000000 | 79.000000 | 88.000000 | 31.000000 | 81.000000 | 0.365000 | 7.000000 | 20.000000 | 0.348000 | 8.000000 | . 25% 2.000000 | 2.000000 | 93.000000 | 96.000000 | 37.000000 | 85.000000 | 0.425000 | 8.000000 | 22.000000 | 0.350000 | 9.000000 | . 50% 3.000000 | 3.000000 | 100.000000 | 105.000000 | 38.000000 | 87.000000 | 0.434000 | 8.000000 | 22.000000 | 0.364000 | 9.000000 | . 75% 4.000000 | 4.000000 | 106.000000 | 114.000000 | 42.000000 | 88.000000 | 0.469000 | 9.000000 | 23.000000 | 0.409000 | 20.000000 | . max 5.000000 | 5.000000 | 115.000000 | 117.000000 | 43.000000 | 99.000000 | 0.477000 | 10.000000 | 23.000000 | 0.435000 | 23.000000 | . Merge multi index headers and remove unwanted tags . In stead of indexing by columns with this notation, . sac_2017_df[(&#39;Unnamed: 0_level_0&#39;, &#39;Rk&#39;)] . we need to merge the header columns to allow for this type of indexing . sac_2017_df[&#39;Rk&#39;] . Lets do a quick magic wave of the hand and merge these headers together . Before: . sac_2017_df.columns[5:15] . MultiIndex([(&#39;Unnamed: 5_level_0&#39;, &#39;W/L&#39;), (&#39;Unnamed: 6_level_0&#39;, &#39;Tm&#39;), (&#39;Unnamed: 7_level_0&#39;, &#39;Opp&#39;), ( &#39;Team&#39;, &#39;FG&#39;), ( &#39;Team&#39;, &#39;FGA&#39;), ( &#39;Team&#39;, &#39;FG%&#39;), ( &#39;Team&#39;, &#39;3P&#39;), ( &#39;Team&#39;, &#39;3PA&#39;), ( &#39;Team&#39;, &#39;3P%&#39;), ( &#39;Team&#39;, &#39;FT&#39;)], ) . merged_columns = sac_2017_df.columns.map(&#39;.&#39;.join) . . After: . merged_columns[5:15] . . Index([&#39;Unnamed: 5_level_0.W/L&#39;, &#39;Unnamed: 6_level_0.Tm&#39;, &#39;Unnamed: 7_level_0.Opp&#39;, &#39;Team.FG&#39;, &#39;Team.FGA&#39;, &#39;Team.FG%&#39;, &#39;Team.3P&#39;, &#39;Team.3PA&#39;, &#39;Team.3P%&#39;, &#39;Team.FT&#39;], dtype=&#39;object&#39;) . . Note: Lets break that piece of code above down for a sec: sac_2017_df.columns.map(&#8217;.&#8217;.join) is calling the str.join() function where the str is &#8217;.&#8217; for each column with the .map() function . Now with the columns merged, we can keep the prefixed descriptions such as Team and Opponent so we know whose stats we&#39;re viewing but prefixes like &#39;Unnamed: 0_level_0&#39; are no use to us. . We can use regular expressions to remove the unneeded text in some of our column names . sac_2017_df.columns = merged_columns.str.replace(r&quot;Unnamed: [0-9]_level_[0-9].&quot;, &#39;&#39;, regex=True) . sac_2017_df.iloc[0:5, 0:15] . Rk G Date Unnamed: 3_level_1 Opp W/L Tm Opp Team.FG Team.FGA Team.FG% Team.3P Team.3PA Team.3P% Team.FT . 0 1 | 1 | 2017-10-18 | NaN | HOU | L | 100 | 105 | 42 | 88 | 0.477 | 8 | 23 | 0.348 | 8 | . 1 2 | 2 | 2017-10-20 | @ | DAL | W | 93 | 88 | 37 | 87 | 0.425 | 10 | 23 | 0.435 | 9 | . 2 3 | 3 | 2017-10-21 | @ | DEN | L | 79 | 96 | 31 | 85 | 0.365 | 8 | 22 | 0.364 | 9 | . 3 4 | 4 | 2017-10-23 | @ | PHO | L | 115 | 117 | 43 | 99 | 0.434 | 9 | 22 | 0.409 | 20 | . 4 5 | 5 | 2017-10-26 | NaN | NOP | L | 106 | 114 | 38 | 81 | 0.469 | 7 | 20 | 0.350 | 23 | . There is still an&#39;Unnmaed: 3_level_1&#39; tag after the regex processing which represents if the team of interest was playing home or away. We won&#39;t even be using this column as is so we can just process our new column and drop &#39;Unnamed: 3_level_1&#39; after. . The existing column consists of discreet values &#39;NaN&#39; or @ indictation if the team was playing at home or away for this instance. We can simply check if the row value is NaN using the .isnull() function in pandas and set those values as a new column . sac_2017_df[&#39;playing_home&#39;] = sac_2017_df[&#39;Unnamed: 3_level_1&#39;].isnull() . Now that we have our column we can simply drop the existing &quot;Unnamed: 3_level_1&quot; column because &quot;playing_home&quot; represents the same thing now but with true and false values . sac_2017_df.drop(columns=[&#39;Unnamed: 3_level_1&#39;], inplace=True) . sac_2017_df.iloc[0:5, 0:15] . Rk G Date Unnamed: 3_level_1 Opp W/L Tm Opp Team.FG Team.FGA Team.FG% Team.3P Team.3PA Team.3P% Team.FT . 0 1 | 1 | 2017-10-18 | NaN | HOU | L | 100 | 105 | 42 | 88 | 0.477 | 8 | 23 | 0.348 | 8 | . 1 2 | 2 | 2017-10-20 | @ | DAL | W | 93 | 88 | 37 | 87 | 0.425 | 10 | 23 | 0.435 | 9 | . 2 3 | 3 | 2017-10-21 | @ | DEN | L | 79 | 96 | 31 | 85 | 0.365 | 8 | 22 | 0.364 | 9 | . 3 4 | 4 | 2017-10-23 | @ | PHO | L | 115 | 117 | 43 | 99 | 0.434 | 9 | 22 | 0.409 | 20 | . 4 5 | 5 | 2017-10-26 | NaN | NOP | L | 106 | 114 | 38 | 81 | 0.469 | 7 | 20 | 0.350 | 23 | . In order to prepare this data for a logistic regression model, we will also need to convert the non-numeric columns we plan to use to numerical values. Specifically converting the column of interest &quot;W/L&quot; to a numeric representation . sac_2017_df[&#39;dub&#39;] = sac_2017_df[&#39;W/L&#39;] == &#39;W&#39; . True values in this new column represent the team of interest got the dub or the Wu as Mastah Killah would say #WuTang #ATLUnited . sac_2017_df.iloc[0:5, -10:] . Opponent.FTA Opponent.FT% Opponent.ORB Opponent.TRB Opponent.AST Opponent.STL Opponent.BLK Opponent.TOV Opponent.PF dub . 0 29 | 0.931 | 12 | 44 | 19 | 7 | 3 | 14 | 14 | False | . 1 21 | 0.714 | 7 | 36 | 19 | 8 | 7 | 12 | 13 | True | . 2 20 | 0.600 | 18 | 58 | 25 | 7 | 2 | 16 | 19 | False | . 3 27 | 0.852 | 6 | 45 | 20 | 6 | 5 | 20 | 25 | False | . 4 23 | 0.739 | 10 | 47 | 22 | 5 | 3 | 15 | 24 | False | . Might as well make a pipeline . We have established, at least, our first pass at preparing the dataset. Since we will have to prepare the other dataframes in a similar way we can mitigate this by creating a data pipeline. This pipeline will take each original dataframe in and run the same preprocessing steps. This ensures everything is going through the same steps. Pipelines are not required but it will help you to stay organized . To make a pipeline we&#39;ll need to make the previous steps we created into a function to pass each dataframe through . def data_pipeline(df): test = df.columns.map(&#39;.&#39;.join).str.strip(&#39;.&#39;) df.columns = test.str.replace(r&quot;Unnamed: [0-9]_level_[0-9].&quot;, &#39;&#39;, regex=True) df[&#39;playing_home&#39;] = df[&#39;Unnamed: 3_level_1&#39;].isnull() df.drop(columns=[&#39;Unnamed: 3_level_1&#39;], inplace=True) df[&#39;dub&#39;] = df[&#39;W/L&#39;] == &#39;W&#39; df.drop(columns=[&#39;W/L&#39;], inplace=True) return df . Running the pipeline . We can consolidate the number of duplicate lines to run into a function and process all similar datasets with the same function. The code below reads in each dataset and immediately uses the pandas .pipe() function passing in the preprocessing function. Though we didn&#39;t use it, the pandas.DataFrame.pipe() function allows positional and keyword arguments to be passed in with the function to run. . sac_2017_df = pd.read_csv(sac_2017_szn, header=[0, 1]).pipe(data_pipeline) sac_2018_df = pd.read_csv(sac_2018_szn, header=[0, 1]).pipe(data_pipeline) mil_2017_df = pd.read_csv(mil_2017_szn, header=[0, 1]).pipe(data_pipeline) mil_2018_df = pd.read_csv(mil_2018_szn, header=[0, 1]).pipe(data_pipeline) . The Jist . Data cleaning and preprocessing is not the most fun job in data science but it sets the foundation for whatever model that will be used. There are a number of automated tools and software that claim to automaticlly clean datasets but from this notebook it&#39;s easy to see it isn&#39;t a cookie cutter process. This dataset was cleaned knowing a linear regression model was going to be used and the tags to fit the model but various other methods could have been used to clean this data. Such as scaling the dataset or one-hot encoding all string columns (but I plan to not use most of them so I didn&#39;t bother). An experienced engineer onced told me coding should be the easy part. That staement didn&#39;t hit me at first but I now understand that statement speaks to the impoartance of understanding what you want to do with the data or a model. Speaking of model, in the next post we&#39;ll get right into linear regression models and how to measure their success cause .",
            "url": "https://dpendleton22.github.io/valuebyerror/data%20cleaning/data%20preparation/pandas.pipe/nba/2020/01/18/mil_sac_data_prep.html",
            "relUrl": "/data%20cleaning/data%20preparation/pandas.pipe/nba/2020/01/18/mil_sac_data_prep.html",
            "date": " • Jan 18, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Exploratory Data Analysis",
            "content": "Exploratory Data Analysis . Originally created: Feb 12, 2019 . TL;DR . Exploratory analysis is an underrated process that allows us to gain some critical knowledge about the data to discover trends and outliers. You need to explore your data before you try and predict your data . What is this article and what will you show me? . For quite some time I’ve been meaning to jump into the realm of sports analytics but there were way too many things to take into account and five-thirty-eight is killing’ the game so why even bother? Because it’s real out here and I still have questions that are not answered, and I aint going to Sway for the answers. . . So who better than myself. My plan is to provide analytics for teams that interest me. There’s no point in choosing my LeBron-less Cavs since we all know what fate lies ahead for them but as Joel Embiid said we’re still the defending eastern conference champs! No, instead I chose two teams that aren’t favorites to win the finals but provide enough interest to study their playing style. Those teams are the Milwaukee Bucks and the Sacramento Kings…… . ”Imma let you finish David, you had me with the Bucks but the Kings my dude….really?” . -Yes, really, the Sacramento Kings are a team to watch for. . There’s really no need to explain the Bucks as they have the Greek Freak, Giannis Antetokounmpo. This kid, yes I said kid (he was born in 1994!) is 6’11 and 245 lbs (sweet Jesus this kid is a freak). His time with he Bucks was eventually going to come and with LeBron leaving that time might be now. The Milwaukee Bucks (MIL) and Sacramento Kings (SAC) were initially selected from my knowledge of watching basketball over the years with no analytic knowledge to backup my initial selections. In an effort to support these claims, let’s do some exploratory analysis to backup my initial selections . In this article I will show: . Two exploratory analysis plots, why and when to use them | Why my selections for SAC and MIL are valid numerically | The conducive information that can be analyzed from exploratory data | . Lets jump right into an imperative figure for summary detail, the box plot. . The box plot . What’s the jist? . Observing the overall percentage distribution of the dataset will give useful insight of where the subject of interest lies especially when there is a benchmark | . Some people think you need have have your programming skills at a Bruce Lee level to do analytics or machine learning, I believe a primary skill needed is an imagination and ability to ask the right questions . Why is that? I’ll write a 2-piece on this later but how else do you know what you’re going to code analytically if you don’t know what information you want? Just follow along for a sec and you’ll see where I’m getting at. Before I get into analyzing MIL or SAC I need to know where they sit in the league. This article will just observe the general stats (assists, turnovers, field goal percentage, etc) before getting into the weeds with the advanced analytics that are becoming standard in the league. To start off let’s do a little underhand pitch on this: . How well did MIL and SAC average amongst the entire league last year, 2017-2018? . Before we start getting jiggy with it, what are some things we already know about what happened last year (2017-2018)? GSW shot the lights out! This KD added team terrorized the entire NBA and made a snooze fest out of the regular and post season, even J.R. Smith went to sleep during the finals . . So to answer this question we need the right type of plot to give us a quick glance of the entire NBA league over the season. This calls for a box plot summary of the stats we wish to view. A quick summary for those who dosed off during stats about box plots and when they’re useful. A box plot provides a graphical percentage summary of the distribution of the data. With this information in mind, I’m gonna guess GSW are at the top of a shooting percentage (FG and/or 3P). Knowing where the 2-time NBA champs sat gives us a benchmark to compare how SAC and MIL are with those same stats. . Basketball Metrics . Hold up, wait a minute… . Before we jump stop into this box plot, lets take a quick euro step to learn about the various measured stats in this article. . . Shoutout to the WNBA for this one #WatchMeWork . NBA stats have two distinct categories, baisc and advanced. All the stats used in this article are categorized as basic stats; which are stats that can be determined in-game. Such as keeping track of a teams points or how many blocks they have. Advanced stats is a newer trend in the field and is where things get a little, well…advanced. They can include stats such as player ratings or player efficiency. These advanced stats are developed calculations that are derived by using the basic stats in the alogorithms. I’ll dive into the different advanced stats in another article but for the basic stats you’ll see the following: . FG% - Field goal percenatge | 3P% - Three point percentage | FT% - Free throw percentage | AST - Total assists | BLK - Total blocks | ORB - Total offensive rebounds | TRB - Total rebounds, offensive and defensive | TOV - Total turnovers | . All basic stats in this article are on a team level but they can also be used to measure an individual player’s performance. . NBA 2017 - 2018 League Summary . . What is the take away from observing the 2017-2018 season overall? . Though 40% does not initially sound great, having ~35-37% 3P% shooting is great for a team and SAC was pretty efficient from the 3 point line last year | Teams usually average between 40 - 50 rebounds for a game with most of them being defensive rebounds | There is a wide range for the FT% but teams still averaged more than 70% for the season | MIL had the lowest total rebounds in the league | SAC was surprisingly efficient from the 3 point line Now that we got a glance at last season, let’s see how SAC and MIL are doing this current NBA season: | . . What is the current 2018-2019 season takeaway? . SAC is still efficient from the 3 point line and increased their field goal percentage from the court | MIL went from the worst total rebounding team to the league’s best for the season | MIL and SAC are moving the ball more efficiently with significant increases in AST | . The box plot is great as a quick glance to see the trends within the data but there’s another plot that will slice the data into an even finer exploration for analysis . A histogram will show how frequently a specific stat occured, and with a little extra sauce we can split that data to view home and away wins and losses. . . Shouout to Giannis with the extra sauce for this post . The Histogram . What’s the jist? . Using a histogram with a little bit of sauce, we’ll be able to see how well a team performed home vs away and if they more frequently won or lost those games | . How well did MIL and SAC play during home and away games? . How to read the facet grid histogram: . The facet grid allows for the data to be sectioned or categorized based on specific categorical values. In this case we sectioned the data off for home and away games (varied by column) In additon I wanted to categorigze games that were won and lost (varied by rows). . So in this article for all the listed facet grid histograms the formatting is as follows: . top left - home games that were won | bottom left - away games that were won | top right, home games that were lost | bottom right - away games that were lost | . Milwuakee Bucks and Rebounding . Of the 32 games the Milwaukee Bucks have managed 42 or more total rebounds they’ve won 75% of those games or 26 games to be exact. The league’s average for team’s total rebounds is 43! This means it is more than probable for this team to increase their team rebounding Note the league FGA, FG and Milwaukee’s FGA, FG (MIL shoots 3 shots less than the league FGA but maintains to be within the league mean of FG with 39) . . MIL is currently second in the eastern conference with a 41-13 record. Last season, though MIL was able to produce decent stats they were the weakest in the league for total rebounds. It was noted when MIL was able to rebound they won 75% of those games. This season with MIL being second in the east and a likely eastern conference contender, they are the top total rebound team in the league. Of the 45 times MIL has been able to produce 43 or more total rebounds they’ve won 34 of those games or 79%. . . Sacramento Kings and Ball Movement . Of the 47 times the Sacramento Kings have been able to produce 23 or more assists they’ve won 74% of those game (35 games to be exact) Though assists may not seem imperative to a team’s production, the defending champions Golden State Warriors led the league in assists with 29.3 which were one of the three outliers from the box plot. The other two outliers were the New Orleans Pelicans and the Philadelphia 76ers (both 2017-2018 playoff teams) . . When observing MIL assists, they only have a 60% increase in wins when producing over 22 assists (28 wins out of 46 games) Last season SAC won 74% of their games (24/34) when they had 23 or more assists (which was their mean assists last year). This year through 55 games that mean has increased to 25 assists over the current season. Of the 34 games SAC has been able to produce 25 or more assists, they’ve won 20 of those games or 58%. The assist increase is directly correlated to their FG% increase from 45 - 47% as an assist is only completed after a made FG. Of the 23 times SAC shot 45 or more percent on the court they’ve won 14 of those games or 60%. . . Sacramento is currently 9th in the Western conference with a 29-23 record and they have two 2019 all-stars in De’Aron Fox and Buddy Hield. They’re a team to lookout for in the next few seasons and even for the second half of this year. . The Jist . Exploring and understanding these basic stats (BLK, AST, etc.) and how they are utilized with each team gives us a better understanding of what stats are important and to which teams. A box plot is useful to determine how well a team is competing compared to the league but in order to get a detail of overall performance a histogram is better. A team such as Sacramento, with a young future star point guard, they are expected to do better with ball movement and setting up efficient shots. Milwaukee having a number of tall and lanky players such as the Greek Freak, Thon Maker and Brook Lopez, they’re expected to not give second shot opportunities to their opponents by rebounding better. With this information we are able to select what tags are imperative in creating a predictive model for the second half of the season. .",
            "url": "https://dpendleton22.github.io/valuebyerror/data%20exploration/box%20plots/histograms/nba/2020/01/14/nba-analysis-post.html",
            "relUrl": "/data%20exploration/box%20plots/histograms/nba/2020/01/14/nba-analysis-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I remember a joke during my undergrate years, “if engineers got everything right the first time they’d be called magicians”. As a foundational engineer I am skilled at correcting myself and learning from my mistakes. Data science is far from perfect but there are plenty of skills I gained as an engineer that I use in my data science job. One major skill is not over reacting from errors and learning how to measure them and not repeat them. ValuebyError is my open notes on the steps I have and continue to take as I gain my skills in the data science field. Posts will include projects I’ve done, lessons learned from projects and general thoughts on things occuring in the data science field. . I spent about 3 years as an engineer with some software experience and made the transition to being a dedicated data scientist. I have 2 years of applied data scinece research experience and a masters in computer science from Georgia Tech. .",
          "url": "https://dpendleton22.github.io/valuebyerror/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://dpendleton22.github.io/valuebyerror/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}