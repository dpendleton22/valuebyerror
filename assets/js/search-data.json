{
  
    
        "post0": {
            "title": "Data Cleaning & Pipelines",
            "content": "The Jist: Data Cleaning is critical before developing a model . The data exploration post showed how to use knowledge about a dataset to interpret information. Since we know how the 2017-2019 seasons went for the Milwuakee Bucks and Sacramento Kings we can now plan out our machine learning problem. The machine learning model will attempt to predict the outcome of an NBA game before it actually occurs. We can start with using a logistic regression model to get a probabalistic output but we can look into other classification models after we give this one a go. This article outlines the most imperative portion of a machine learning project, outlining the problem and preparing the data. . Part 1: Data Exploration . This post is a continuation of the data exploration post where we explored the 2017-2019 seasons for the Milwuakee Bucks and the Sacramento Kings. Feel free to hop out and pop back in if you want to see the data described and explored: . Part 1 post: Data Exporlation with NBA Data . . Set all the necessary paths for the data . The data was provided by https://www.basketball-reference.com/. They are a great source for anyone interested in sports analytics as an intial introduction. I can go into details later within the project to note the importance of detail in sports data. . Using the pathlib library from pandas it&#39;s straightforward getting all the data file names set. Setting a base folder name is a good method to simply call each dataset path by their name. Another method to get each dataset path would be to use the glob library to search the dataset folder for files with csv extensions . DATA_FOLDER = Path(os.getcwd(), &#39;mil_sac_data&#39;) sac_2017_szn = Path(DATA_FOLDER, &#39;sac_2017_2018_szn.csv&#39;) sac_2018_szn = Path(DATA_FOLDER, &#39;sac_2018_2019_szn.csv&#39;) mil_2017_szn = Path(DATA_FOLDER, &#39;mil_2017_2018_szn.csv&#39;) mil_2018_szn = Path(DATA_FOLDER, &#39;mil_2018_2019_szn.csv&#39;) . . Let&#39;s review one of the datasets to determine how they all need to be cleaned . sac_2017_df = pd.read_csv(sac_2017_szn, header=[0,1]) . Hold up, why are you setting the header argument? . Most times than not, calling pd.read_csv(&quot;filename&quot;) with no additional arguments would read in a dataframe as expected. In this instance, BasketballReference provides two headers in their csv so we need to let pandas know in order to process the dataset. Pandas read_csv() function has over 20 arguments that can be set depending on how the data is parsed and organized in the original file. So if your data is a little funky, the function may still be able to handle it. . Pandas read_csv() documentation: pandas.read_csv() . sac_2017_df.iloc[0:5, 0:15] . . Unnamed: 0_level_0 Unnamed: 1_level_0 Unnamed: 2_level_0 Unnamed: 3_level_0 Unnamed: 4_level_0 Unnamed: 5_level_0 Unnamed: 6_level_0 Unnamed: 7_level_0 Team . Rk G Date Unnamed: 3_level_1 Opp W/L Tm Opp FG FGA FG% 3P 3PA 3P% FT . 0 1 | 1 | 2017-10-18 | NaN | HOU | L | 100 | 105 | 42 | 88 | 0.477 | 8 | 23 | 0.348 | 8 | . 1 2 | 2 | 2017-10-20 | @ | DAL | W | 93 | 88 | 37 | 87 | 0.425 | 10 | 23 | 0.435 | 9 | . 2 3 | 3 | 2017-10-21 | @ | DEN | L | 79 | 96 | 31 | 85 | 0.365 | 8 | 22 | 0.364 | 9 | . 3 4 | 4 | 2017-10-23 | @ | PHO | L | 115 | 117 | 43 | 99 | 0.434 | 9 | 22 | 0.409 | 20 | . 4 5 | 5 | 2017-10-26 | NaN | NOP | L | 106 | 114 | 38 | 81 | 0.469 | 7 | 20 | 0.350 | 23 | . . Tip: Always view the dimensions of your data before analyzing it . print (f&quot;This dataset is {len(sac_2017_df)} in length and contains {len(sac_2017_df.columns)} columns&quot;) . This dataset is 82 in length and contains 41 columns . Using df.describe() is an easy and useful way to breifly view the distribution of the dataset across all the columns. This dataset is 82 rows in length which makes sense because there are 82 games in a regular season and contains 41 columns . sac_2017_df.iloc[0:5, 0:15].describe() . . Unnamed: 0_level_0 Unnamed: 1_level_0 Unnamed: 6_level_0 Unnamed: 7_level_0 Team . Rk G Tm Opp FG FGA FG% 3P 3PA 3P% FT . count 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | 5.000000 | . mean 3.000000 | 3.000000 | 98.600000 | 104.000000 | 38.200000 | 88.000000 | 0.434000 | 8.400000 | 22.000000 | 0.381200 | 13.800000 | . std 1.581139 | 1.581139 | 13.612494 | 12.144958 | 4.764452 | 6.708204 | 0.044486 | 1.140175 | 1.224745 | 0.038855 | 7.120393 | . min 1.000000 | 1.000000 | 79.000000 | 88.000000 | 31.000000 | 81.000000 | 0.365000 | 7.000000 | 20.000000 | 0.348000 | 8.000000 | . 25% 2.000000 | 2.000000 | 93.000000 | 96.000000 | 37.000000 | 85.000000 | 0.425000 | 8.000000 | 22.000000 | 0.350000 | 9.000000 | . 50% 3.000000 | 3.000000 | 100.000000 | 105.000000 | 38.000000 | 87.000000 | 0.434000 | 8.000000 | 22.000000 | 0.364000 | 9.000000 | . 75% 4.000000 | 4.000000 | 106.000000 | 114.000000 | 42.000000 | 88.000000 | 0.469000 | 9.000000 | 23.000000 | 0.409000 | 20.000000 | . max 5.000000 | 5.000000 | 115.000000 | 117.000000 | 43.000000 | 99.000000 | 0.477000 | 10.000000 | 23.000000 | 0.435000 | 23.000000 | . Merge multi index headers and remove unwanted tags . In stead of indexing by columns with this notation, . sac_2017_df[(&#39;Unnamed: 0_level_0&#39;, &#39;Rk&#39;)] . we need to merge the header columns to allow for this type of indexing . sac_2017_df[&#39;Rk&#39;] . Lets do a quick magic wave of the hand and merge these headers together . Before: . sac_2017_df.columns[5:15] . MultiIndex([(&#39;Unnamed: 5_level_0&#39;, &#39;W/L&#39;), (&#39;Unnamed: 6_level_0&#39;, &#39;Tm&#39;), (&#39;Unnamed: 7_level_0&#39;, &#39;Opp&#39;), ( &#39;Team&#39;, &#39;FG&#39;), ( &#39;Team&#39;, &#39;FGA&#39;), ( &#39;Team&#39;, &#39;FG%&#39;), ( &#39;Team&#39;, &#39;3P&#39;), ( &#39;Team&#39;, &#39;3PA&#39;), ( &#39;Team&#39;, &#39;3P%&#39;), ( &#39;Team&#39;, &#39;FT&#39;)], ) . merged_columns = sac_2017_df.columns.map(&#39;.&#39;.join) . . After: . merged_columns[5:15] . . Index([&#39;Unnamed: 5_level_0.W/L&#39;, &#39;Unnamed: 6_level_0.Tm&#39;, &#39;Unnamed: 7_level_0.Opp&#39;, &#39;Team.FG&#39;, &#39;Team.FGA&#39;, &#39;Team.FG%&#39;, &#39;Team.3P&#39;, &#39;Team.3PA&#39;, &#39;Team.3P%&#39;, &#39;Team.FT&#39;], dtype=&#39;object&#39;) . . Note: Lets break that piece of code above down for a sec: sac_2017_df.columns.map(&#8217;.&#8217;.join) is calling the str.join() function where the str is &#8217;.&#8217; for each column with the .map() function . Now with the columns merged, we can keep the prefixed descriptions such as Team and Opponent so we know whose stats we&#39;re viewing but prefixes like &#39;Unnamed: 0_level_0&#39; are no use to us. . We can use regular expressions to remove the unneeded text in some of our column names . sac_2017_df.columns = merged_columns.str.replace(r&quot;Unnamed: [0-9]_level_[0-9].&quot;, &#39;&#39;, regex=True) . sac_2017_df.iloc[0:5, 0:15] . Rk G Date Unnamed: 3_level_1 Opp W/L Tm Opp Team.FG Team.FGA Team.FG% Team.3P Team.3PA Team.3P% Team.FT . 0 1 | 1 | 2017-10-18 | NaN | HOU | L | 100 | 105 | 42 | 88 | 0.477 | 8 | 23 | 0.348 | 8 | . 1 2 | 2 | 2017-10-20 | @ | DAL | W | 93 | 88 | 37 | 87 | 0.425 | 10 | 23 | 0.435 | 9 | . 2 3 | 3 | 2017-10-21 | @ | DEN | L | 79 | 96 | 31 | 85 | 0.365 | 8 | 22 | 0.364 | 9 | . 3 4 | 4 | 2017-10-23 | @ | PHO | L | 115 | 117 | 43 | 99 | 0.434 | 9 | 22 | 0.409 | 20 | . 4 5 | 5 | 2017-10-26 | NaN | NOP | L | 106 | 114 | 38 | 81 | 0.469 | 7 | 20 | 0.350 | 23 | . There is still an&#39;Unnmaed: 3_level_1&#39; tag after the regex processing which represents if the team of interest was playing home or away. We won&#39;t even be using this column as is so we can just process our new column and drop &#39;Unnamed: 3_level_1&#39; after. . The existing column consists of discreet values &#39;NaN&#39; or @ indictation if the team was playing at home or away for this instance. We can simply check if the row value is NaN using the .isnull() function in pandas and set those values as a new column . sac_2017_df[&#39;playing_home&#39;] = sac_2017_df[&#39;Unnamed: 3_level_1&#39;].isnull() . Now that we have our column we can simply drop the existing &quot;Unnamed: 3_level_1&quot; column because &quot;playing_home&quot; represents the same thing now but with true and false values . sac_2017_df.drop(columns=[&#39;Unnamed: 3_level_1&#39;], inplace=True) . sac_2017_df.iloc[0:5, 0:15] . Rk G Date Unnamed: 3_level_1 Opp W/L Tm Opp Team.FG Team.FGA Team.FG% Team.3P Team.3PA Team.3P% Team.FT . 0 1 | 1 | 2017-10-18 | NaN | HOU | L | 100 | 105 | 42 | 88 | 0.477 | 8 | 23 | 0.348 | 8 | . 1 2 | 2 | 2017-10-20 | @ | DAL | W | 93 | 88 | 37 | 87 | 0.425 | 10 | 23 | 0.435 | 9 | . 2 3 | 3 | 2017-10-21 | @ | DEN | L | 79 | 96 | 31 | 85 | 0.365 | 8 | 22 | 0.364 | 9 | . 3 4 | 4 | 2017-10-23 | @ | PHO | L | 115 | 117 | 43 | 99 | 0.434 | 9 | 22 | 0.409 | 20 | . 4 5 | 5 | 2017-10-26 | NaN | NOP | L | 106 | 114 | 38 | 81 | 0.469 | 7 | 20 | 0.350 | 23 | . In order to prepare this data for a logistic regression model, we will also need to convert the non-numeric columns we plan to use to numerical values. Specifically converting the column of interest &quot;W/L&quot; to a numeric representation . sac_2017_df[&#39;dub&#39;] = sac_2017_df[&#39;W/L&#39;] == &#39;W&#39; . True values in this new column represent the team of interest got the dub or the Wu as Mastah Killah would say #WuTang #ATLUnited . sac_2017_df.iloc[0:5, -10:] . Opponent.FTA Opponent.FT% Opponent.ORB Opponent.TRB Opponent.AST Opponent.STL Opponent.BLK Opponent.TOV Opponent.PF dub . 0 29 | 0.931 | 12 | 44 | 19 | 7 | 3 | 14 | 14 | False | . 1 21 | 0.714 | 7 | 36 | 19 | 8 | 7 | 12 | 13 | True | . 2 20 | 0.600 | 18 | 58 | 25 | 7 | 2 | 16 | 19 | False | . 3 27 | 0.852 | 6 | 45 | 20 | 6 | 5 | 20 | 25 | False | . 4 23 | 0.739 | 10 | 47 | 22 | 5 | 3 | 15 | 24 | False | . Might as well make a pipeline . We have established, at least, our first pass at preparing the dataset. Since we will have to prepare the other dataframes in a similar way we can mitigate this by creating a data pipeline. This pipeline will take each original dataframe in and run the same preprocessing steps. This ensures everything is going through the same steps. Pipelines are not required but it will help you to stay organized . To make a pipeline we&#39;ll need to make the previous steps we created into a function to pass each dataframe through . def data_pipeline(df): test = df.columns.map(&#39;.&#39;.join).str.strip(&#39;.&#39;) df.columns = test.str.replace(r&quot;Unnamed: [0-9]_level_[0-9].&quot;, &#39;&#39;, regex=True) df[&#39;playing_home&#39;] = df[&#39;Unnamed: 3_level_1&#39;].isnull() df.drop(columns=[&#39;Unnamed: 3_level_1&#39;], inplace=True) df[&#39;dub&#39;] = df[&#39;W/L&#39;] == &#39;W&#39; df.drop(columns=[&#39;W/L&#39;], inplace=True) return df . Running the pipeline . We can consolidate the number of duplicate lines to run into a function and process all similar datasets with the same function. The code below reads in each dataset and immediately uses the pandas .pipe() function passing in the preprocessing function. Though we didn&#39;t use it, the pandas.DataFrame.pipe() function allows positional and keyword arguments to be passed in with the function to run. . sac_2017_df = pd.read_csv(sac_2017_szn, header=[0, 1]).pipe(data_pipeline) sac_2018_df = pd.read_csv(sac_2018_szn, header=[0, 1]).pipe(data_pipeline) mil_2017_df = pd.read_csv(mil_2017_szn, header=[0, 1]).pipe(data_pipeline) mil_2018_df = pd.read_csv(mil_2018_szn, header=[0, 1]).pipe(data_pipeline) . The Jist . Data cleaning and preprocessing is not the most fun job in data science but it sets the foundation for whatever model that will be used. There are a number of automated tools and software that claim to automaticlly clean datasets but from this notebook it&#39;s easy to see it isn&#39;t a cookie cutter process. This dataset was cleaned knowing a linear regression model was going to be used and the tags to fit the model but various other methods could have been used to clean this data. Such as scaling the dataset or one-hot encoding all string columns (but I plan to not use most of them so I didn&#39;t bother). An experienced engineer onced told me coding should be the easy part. That staement didn&#39;t hit me at first but I now understand that statement speaks to the impoartance of understanding what you want to do with the data or a model. Speaking of model, in the next post we&#39;ll get right into linear regression models and how to measure their success cause .",
            "url": "https://dpendleton22.github.io/valuebyerror/data%20cleaning/data%20preparation/pandas.pipe/nba/2020/01/18/mil_sac_data_prep.html",
            "relUrl": "/data%20cleaning/data%20preparation/pandas.pipe/nba/2020/01/18/mil_sac_data_prep.html",
            "date": " • Jan 18, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Exploratory Data Analysis",
            "content": "Exploratory Data Analysis . Originally created: Feb 12, 2019 . TL;DR . Exploratory analysis is an underrated process that allows us to gain some critical knowledge about the data to discover trends and outliers. You need to explore your data before you try and predict your data . What is this article and what will you show me? . For quite some time I’ve been meaning to jump into the realm of sports analytics but there were way too many things to take into account and five-thirty-eight is killing’ the game so why even bother? Because it’s real out here and I still have questions that are not answered, and I aint going to Sway for the answers. . . So who better than myself. My plan is to provide analytics for teams that interest me. There’s no point in choosing my LeBron-less Cavs since we all know what fate lies ahead for them but as Joel Embiid said we’re still the defending eastern conference champs! No, instead I chose two teams that aren’t favorites to win the finals but provide enough interest to study their playing style. Those teams are the Milwaukee Bucks and the Sacramento Kings…… . ”Imma let you finish David, you had me with the Bucks but the Kings my dude….really?” . -Yes, really, the Sacramento Kings are a team to watch for. . There’s really no need to explain the Bucks as they have the Greek Freak, Giannis Antetokounmpo. This kid, yes I said kid (he was born in 1994!) is 6’11 and 245 lbs (sweet Jesus this kid is a freak). His time with he Bucks was eventually going to come and with LeBron leaving that time might be now. The Milwaukee Bucks (MIL) and Sacramento Kings (SAC) were initially selected from my knowledge of watching basketball over the years with no analytic knowledge to backup my initial selections. In an effort to support these claims, let’s do some exploratory analysis to backup my initial selections . In this article I will show: . Two exploratory analysis plots, why and when to use them | Why my selections for SAC and MIL are valid numerically | The conducive information that can be analyzed from exploratory data | . Lets jump right into an imperative figure for summary detail, the box plot. . The box plot . What’s the jist? . Observing the overall percentage distribution of the dataset will give useful insight of where the subject of interest lies especially when there is a benchmark | . Some people think you need have have your programming skills at a Bruce Lee level to do analytics or machine learning, I believe a primary skill needed is an imagination and ability to ask the right questions . Why is that? I’ll write a 2-piece on this later but how else do you know what you’re going to code analytically if you don’t know what information you want? Just follow along for a sec and you’ll see where I’m getting at. Before I get into analyzing MIL or SAC I need to know where they sit in the league. This article will just observe the general stats (assists, turnovers, field goal percentage, etc) before getting into the weeds with the advanced analytics that are becoming standard in the league. To start off let’s do a little underhand pitch on this: . How well did MIL and SAC average amongst the entire league last year, 2017-2018? . Before we start getting jiggy with it, what are some things we already know about what happened last year (2017-2018)? GSW shot the lights out! This KD added team terrorized the entire NBA and made a snooze fest out of the regular and post season, even J.R. Smith went to sleep during the finals . . So to answer this question we need the right type of plot to give us a quick glance of the entire NBA league over the season. This calls for a box plot summary of the stats we wish to view. A quick summary for those who dosed off during stats about box plots and when they’re useful. A box plot provides a graphical percentage summary of the distribution of the data. With this information in mind, I’m gonna guess GSW are at the top of a shooting percentage (FG and/or 3P). Knowing where the 2-time NBA champs sat gives us a benchmark to compare how SAC and MIL are with those same stats. . Basketball Metrics . Hold up, wait a minute… . Before we jump stop into this box plot, lets take a quick euro step to learn about the various measured stats in this article. . . Shoutout to the WNBA for this one #WatchMeWork . NBA stats have two distinct categories, baisc and advanced. All the stats used in this article are categorized as basic stats; which are stats that can be determined in-game. Such as keeping track of a teams points or how many blocks they have. Advanced stats is a newer trend in the field and is where things get a little, well…advanced. They can include stats such as player ratings or player efficiency. These advanced stats are developed calculations that are derived by using the basic stats in the alogorithms. I’ll dive into the different advanced stats in another article but for the basic stats you’ll see the following: . FG% - Field goal percenatge | 3P% - Three point percentage | FT% - Free throw percentage | AST - Total assists | BLK - Total blocks | ORB - Total offensive rebounds | TRB - Total rebounds, offensive and defensive | TOV - Total turnovers | . All basic stats in this article are on a team level but they can also be used to measure an individual player’s performance. . NBA 2017 - 2018 League Summary . . What is the take away from observing the 2017-2018 season overall? . Though 40% does not initially sound great, having ~35-37% 3P% shooting is great for a team and SAC was pretty efficient from the 3 point line last year | Teams usually average between 40 - 50 rebounds for a game with most of them being defensive rebounds | There is a wide range for the FT% but teams still averaged more than 70% for the season | MIL had the lowest total rebounds in the league | SAC was surprisingly efficient from the 3 point line Now that we got a glance at last season, let’s see how SAC and MIL are doing this current NBA season: | . . What is the current 2018-2019 season takeaway? . SAC is still efficient from the 3 point line and increased their field goal percentage from the court | MIL went from the worst total rebounding team to the league’s best for the season | MIL and SAC are moving the ball more efficiently with significant increases in AST | . The box plot is great as a quick glance to see the trends within the data but there’s another plot that will slice the data into an even finer exploration for analysis . A histogram will show how frequently a specific stat occured, and with a little extra sauce we can split that data to view home and away wins and losses. . . Shouout to Giannis with the extra sauce for this post . The Histogram . What’s the jist? . Using a histogram with a little bit of sauce, we’ll be able to see how well a team performed home vs away and if they more frequently won or lost those games | . How well did MIL and SAC play during home and away games? . How to read the facet grid histogram: . The facet grid allows for the data to be sectioned or categorized based on specific categorical values. In this case we sectioned the data off for home and away games (varied by column) In additon I wanted to categorigze games that were won and lost (varied by rows). . So in this article for all the listed facet grid histograms the formatting is as follows: . top left - home games that were won | bottom left - away games that were won | top right, home games that were lost | bottom right - away games that were lost | . Milwuakee Bucks and Rebounding . Of the 32 games the Milwaukee Bucks have managed 42 or more total rebounds they’ve won 75% of those games or 26 games to be exact. The league’s average for team’s total rebounds is 43! This means it is more than probable for this team to increase their team rebounding Note the league FGA, FG and Milwaukee’s FGA, FG (MIL shoots 3 shots less than the league FGA but maintains to be within the league mean of FG with 39) . . MIL is currently second in the eastern conference with a 41-13 record. Last season, though MIL was able to produce decent stats they were the weakest in the league for total rebounds. It was noted when MIL was able to rebound they won 75% of those games. This season with MIL being second in the east and a likely eastern conference contender, they are the top total rebound team in the league. Of the 45 times MIL has been able to produce 43 or more total rebounds they’ve won 34 of those games or 79%. . . Sacramento Kings and Ball Movement . Of the 47 times the Sacramento Kings have been able to produce 23 or more assists they’ve won 74% of those game (35 games to be exact) Though assists may not seem imperative to a team’s production, the defending champions Golden State Warriors led the league in assists with 29.3 which were one of the three outliers from the box plot. The other two outliers were the New Orleans Pelicans and the Philadelphia 76ers (both 2017-2018 playoff teams) . . When observing MIL assists, they only have a 60% increase in wins when producing over 22 assists (28 wins out of 46 games) Last season SAC won 74% of their games (24/34) when they had 23 or more assists (which was their mean assists last year). This year through 55 games that mean has increased to 25 assists over the current season. Of the 34 games SAC has been able to produce 25 or more assists, they’ve won 20 of those games or 58%. The assist increase is directly correlated to their FG% increase from 45 - 47% as an assist is only completed after a made FG. Of the 23 times SAC shot 45 or more percent on the court they’ve won 14 of those games or 60%. . . Sacramento is currently 9th in the Western conference with a 29-23 record and they have two 2019 all-stars in De’Aron Fox and Buddy Hield. They’re a team to lookout for in the next few seasons and even for the second half of this year. . The Jist . Exploring and understanding these basic stats (BLK, AST, etc.) and how they are utilized with each team gives us a better understanding of what stats are important and to which teams. A box plot is useful to determine how well a team is competing compared to the league but in order to get a detail of overall performance a histogram is better. A team such as Sacramento, with a young future star point guard, they are expected to do better with ball movement and setting up efficient shots. Milwaukee having a number of tall and lanky players such as the Greek Freak, Thon Maker and Brook Lopez, they’re expected to not give second shot opportunities to their opponents by rebounding better. With this information we are able to select what tags are imperative in creating a predictive model for the second half of the season. .",
            "url": "https://dpendleton22.github.io/valuebyerror/data%20exploration/box%20plots/histograms/nba/2020/01/14/nba-analysis-post.html",
            "relUrl": "/data%20exploration/box%20plots/histograms/nba/2020/01/14/nba-analysis-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I remember a joke during my undergrate years, “if engineers got everything right the first time they’d be called magicians”. As a foundational engineer I am skilled at correcting myself and learning from my mistakes. Data science is far from perfect but there are plenty of skills I gained as an engineer that I use in my data science job. One major skill is not over reacting from errors and learning how to measure them and not repeat them. ValuebyError is my open notes on the steps I have and continue to take as I gain my skills in the data science field. Posts will include projects I’ve done, lessons learned from projects and general thoughts on things occuring in the data science field. . I spent about 3 years as an engineer with some software experience and made the transition to being a dedicated data scientist. I have 2 years of applied data scinece research experience and a masters in computer science from Georgia Tech. .",
          "url": "https://dpendleton22.github.io/valuebyerror/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://dpendleton22.github.io/valuebyerror/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}